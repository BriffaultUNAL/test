



class Perro:
    especie = 'mam√≠fero'

    def __init__(self, nombre, raza):
        print(f"Creando perro {nombre}, {raza}")

        self.nombre = nombre
        self.raza = raza

    def ladra(self):
        print("Guau")

    def camina(self, pasos):
        print(f"Caminando {pasos} pasos")
        
mi_perro = Perro("Toby", "Bulldog")
mi_perro.ladra()
mi_perro.camina(10)

class Coffee:
        def __init__(self, name, price):
                self.name = name
                self.price = float(price)
        def check_budget(self, budget):
                if not isinstance(budget, (int, float)):
                        print('Enter float or int')
                        exit()
                if budget < 0: 
                    print('Sorry you don\'t have money') 
                    exit() 
        def get_change(self, budget):
                return budget - self.price
        
        def sell(self, budget):
                self.check_budget(budget)
                if budget >= self.price:
                        print(f'You can buy the {self.name} coffee')
                        if budget == self.price:
                                print('It\'s complete')
                        else:
                                print(f'Here is your change {self.get_change(budget)}$')

                        exit('Thanks for your transaction')        
                        
#df.loc[lambda X:X.Num_Estudiantes == 60]        

def measure_time(function):
    def wrapper(*args, **kwargs):
        import time

        start = time.time()
        result = function(*args, **kwargs)
        total = time.time() - start
        print(total, 'seconds' )
        return result

    return wrapper

#*args, **kwargs argunmentos dinamicos

@measure_time
def suma(a, b):
    import time
    time.sleep(1)
    return a + b

print(suma(10, 20))     

"""Paso	Detalle / Por qu√© es relevante
1 ‚Äì Imports	Pipeline y ColumnTransformer aseguran que el mismo pre-procesamiento se aplique en entrenamiento y test, evitando data leakage.
2 ‚Äì Simulaci√≥n	Creamos un patr√≥n realista: semanas c√≠clicas + campa√±a. Generar datos sint√©ticos agiliza pruebas sin exponer datos sensibles.
3 ‚Äì Features	- dow y month capturan estacionalidad.
- is_q_end modela picos de fin de mes.
Las fechas casi siempre requieren ingenier√≠a de calendario.
4 ‚Äì Split temporal	En series de tiempo nunca barajes las filas; entrenas en pasado y pruebas en futuro. Por simplicidad usamos hold-out (30 d√≠as).
5 ‚Äì Pre-procesamiento	- OneHotEncoder transforma categ√≥ricas en dummies; drop="first" evita multicolinealidad.
- RandomForest: buen baseline porque maneja no linealidad e interacciones sin escalar.
6 ‚Äì Entrenamiento	.fit ajusta tanto el encoder como el modelo dentro de la misma tuber√≠a ‚Üí una sola llamada.
7 ‚Äì Evaluaci√≥n	MAE (error absoluto medio) es f√°cil de explicar: ‚Äúfallamos en ¬± X llamadas diarias‚Äù. Puedes comparar con tu margen de planificaci√≥n.
8 ‚Äì Importancia	Suma a 1; indica peso relativo de cada categor√≠a (p. ej. si dow_0 ‚âÉ 0.3, el lunes es crucial en la predicci√≥n). Comunica insights a Workforce Management.   
"""
# 1. ‚îÄ‚îÄ‚îÄ Imports b√°sicos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# 2. ‚îÄ‚îÄ‚îÄ Simulamos 6 meses de datos diarios ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
rng = pd.date_range(start="2025-01-01", end="2025-06-30", freq="D")
n = len(rng)
df = pd.DataFrame({"date": rng})

# patr‚Äà√≥n semanal: lunes pico, fines de semana baja
base_calls = 200 + 50*np.sin(2*np.pi*df["date"].dt.dayofweek/7)

# campa√±a de marketing en marzo eleva las llamadas
campaign = ((df["date"].dt.month == 3).astype(int) * 60)

# ruido aleatorio (Poisson aprox. llamadas)
df["calls"] = (base_calls + campaign + np.random.normal(0, 10, n)).round().astype(int)

# 3. ‚îÄ‚îÄ‚îÄ Feature engineering m√≠nimo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df["dow"]      = df["date"].dt.dayofweek          # 0=lun ‚Ä¶ 6=dom
df["month"]    = df["date"].dt.month
df["is_q_end"] = (df["date"].dt.day > 25).astype(int)  # fin de mes

X = df[["dow", "month", "is_q_end"]]
y = df["calls"]

# 4. ‚îÄ‚îÄ‚îÄ Split train / test temporal (√∫ltimos 30 d√≠as = test) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X_train, X_test = X.iloc[:-30], X.iloc[-30:]
y_train, y_test = y.iloc[:-30], y.iloc[-30:]

# 5. ‚îÄ‚îÄ‚îÄ Pre-proceso: One-Hot a categ√≥ricas + modelo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cat_features = ["dow", "month", "is_q_end"]
preproc = ColumnTransformer(
    [("ohe", OneHotEncoder(drop="first"), cat_features)],
    remainder="passthrough"  # no hay num√©ricas extra, pero ilustrativo
)

model = RandomForestRegressor(
    n_estimators=300, random_state=42)  # √°rbol ensamble robusto

pipe = Pipeline(
    steps=[("prep", preproc),
           ("rf",   model)]
)

# 6. ‚îÄ‚îÄ‚îÄ Entrenamiento ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
pipe.fit(X_train, y_train)

# 7. ‚îÄ‚îÄ‚îÄ Predicci√≥n y evaluaci√≥n ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
y_pred = pipe.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE en test: {mae:.1f} llamadas/d√≠a")

# 8. ‚îÄ‚îÄ‚îÄ (Opcional) importancia de variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
importances = pipe.named_steps["rf"].feature_importances_
feature_names = pipe.named_steps["prep"].get_feature_names_out()
print(pd.Series(importances, index=feature_names).sort_values(ascending=False))        


"""Paso	Qu√© hace / Por qu√© es importante
1 ‚Äì Imports	Incluimos ColumnTransformer y Pipeline para empaquetar todo el flujo (evita data leakage).
2 ‚Äì Dataset sint√©tico	- Reproduce causas reales de variaci√≥n (tipo de llamada, experiencia, etc.).
- A√±adimos ruido para emular variabilidad natural.
3 ‚Äì X & y	Mantener las features separadas del target es crucial para evitar errores al escalar/encodear.
4 ‚Äì Split de validaci√≥n	Usamos stratify en call_type para conservar proporciones entre train y test.
En la vida real, con timestamp usar√≠as un split temporal.
5 ‚Äì Pre-procesamiento	- One-Hot Encoding convierte categ√≥ricas en columnas 0/1.
- StandardScaler centra y escala num√©ricas (GBR no lo exige, pero facilita comparaciones e importa en otros modelos).
6 ‚Äì Modelo	GradientBoostingRegressor captura interacciones y no linealidad (e.g., impacto distinto del ivr_steps seg√∫n call_type).
7 ‚Äì Entrenar	pipe.fit ajusta todo de una vez: primero procesa X, luego entrena el modelo ‚Üí misma l√≥gica en predict().
8 ‚Äì MAE	Error absoluto en segundos es intuitivo para supervisores: ‚Äúfallamos ¬±24 s por llamada promedio‚Äù. Cambia a RMSE si necesitas penalizar m√°s los errores grandes.
9 ‚Äì Importancia	Muestra las features top: tip ‚Üí agrupa columnas dummy del mismo campo para una vista de negocio (‚Äúcall_type explica ‚âà40 % de la predicci√≥n‚Äù)."""


# 1. ‚îÄ‚îÄ‚îÄ Imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error

# 2. ‚îÄ‚îÄ‚îÄ Creamos un dataset sint√©tico de 5 000 llamadas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
n = 5_000
rng = np.random.default_rng(42)

df = pd.DataFrame({
    "call_type": rng.choice(["billing", "tech", "upgrade"], n, p=[.5, .3, .2]),
    "customer_tier": rng.choice(["VIP", "Regular"], n, p=[.15, .85]),
    "agent_tenure_months": rng.integers(1, 60, n),     # experiencia agente
    "ivr_steps": rng.integers(1, 8, n),                # men√∫s recorridos
    "first_call": rng.choice([0, 1], n, p=[.7, .3])    # primer contacto del d√≠a
})

# Duraci√≥n base (seg) seg√∫n tipo + ruido y efectos
base = (df["call_type"].map({"billing": 300, "tech": 420, "upgrade": 500})
        + df["ivr_steps"] * 15                       # cada men√∫ a√±ade ~15 s
        - df["agent_tenure_months"] * 2              # agentes expertos m√°s r√°pidos
        + df["customer_tier"].map({"VIP": -30, "Regular": 0})
        + df["first_call"]  * 40                     # clientes nuevos tardan m√°s
        + rng.normal(0, 25, n))                      # ruido gaussiano

df["duration_sec"] = base.round().astype(int)

# 3. ‚îÄ‚îÄ‚îÄ Seleccionamos features (X) y target (y) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X = df.drop(columns="duration_sec")
y = df["duration_sec"]

# 4. ‚îÄ‚îÄ‚îÄ Hold-out temporal (√∫ltimos 10 %) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# (si tuvieras timestamp real usar√≠as split por fecha)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.10, random_state=42, stratify=df["call_type"]
)

# 5. ‚îÄ‚îÄ‚îÄ Pre-proceso: OHE para categ√≥ricas, escala num√©ricas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cat_cols = ["call_type", "customer_tier", "first_call"]
num_cols = ["agent_tenure_months", "ivr_steps"]

preproc = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first"), cat_cols),
        ("num", StandardScaler(),            num_cols)
    ]
)

# 6. ‚îÄ‚îÄ‚îÄ Modelo: Gradient Boosting (maneja no linealidad & interacci√≥n) ‚îÄ
gbr = GradientBoostingRegressor(random_state=42)

pipe = Pipeline(steps=[("prep", preproc),
                      ("gbr",  gbr)])

# 7. ‚îÄ‚îÄ‚îÄ Entrenar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
pipe.fit(X_train, y_train)

# 8. ‚îÄ‚îÄ‚îÄ Evaluar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
y_pred = pipe.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae:.1f} s ‚Äî error medio absoluto (AHT)")

# 9. ‚îÄ‚îÄ‚îÄ Importancia global de variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
importances = pipe.named_steps["gbr"].feature_importances_
feat_names  = pipe.named_steps["prep"].get_feature_names_out()
print(pd.Series(importances, index=feat_names)
        .sort_values(ascending=False)
        .head(8))


"""Paso	Qu√© hace / Concepto clave
1 ‚Äì Imports	RandomForestClassifier gestiona relaciones no lineales y requiere poco tuning; class_weight="balanced" corrige el desbalance t√≠pico (pocos abandonos).
2 ‚Äì Dataset sint√©tico	Incluimos variables con causalidad plausible: ausencias recientes, turnos nocturnos, tiempo desde el √∫ltimo aumento, etc. prob_quit mezcla reglas simples para generar la etiqueta churn_risk.
3 ‚Äì Separaci√≥n X / y	Mantener target aparte evita filtrarlo accidentalmente en los features.
4 ‚Äì train_test_split estratificado	Se asegura que la proporci√≥n de churners sea la misma en ambos conjuntos ‚áí estimaci√≥n m√°s estable.
5 ‚Äì ColumnTransformer	- One-Hot Encoding: convierte categor√≠as en columnas binarias (elimina problema de orden).
- StandardScaler: centra/escala num√©ricas ‚Üí mejora estabilidad del bosque y es obligatorio si luego pruebas modelos lineales.
6 ‚Äì Modelo	- 400 √°rboles reducen varianza.
- class_weight distribuye ‚Äúpeso‚Äù extra a la clase minoritaria (churn=1) y aumenta recall.
7 ‚Äì Entrenamiento	.fit aplica el pipeline completo en un solo llamado ‚áí evita data-leakage (el scaler s√≥lo ‚Äúve‚Äù train).
8 ‚Äì Evaluaci√≥n	- ROC-AUC: probabilidad de que el modelo rankee al azar dos agentes (uno churner, uno no) correctamente; > 0.75 ya es valioso.
- classification_report imprime precisi√≥n, recall, F1 de cada clase ‚Üí revisa que el recall de churn ‚â• 0.6 (mejor capturar la mayor√≠a de riesgos).
9 ‚Äì Importancia de features	Comunicas al HR que ‚Äúlas ausencias en los √∫ltimos 90 d√≠as y el desempe√±o bajo explican el 45 % del riesgo‚Äù, facilitando plan de acci√≥n."""


import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report

# 2. ‚îÄ‚îÄ‚îÄ Dataset sint√©tico de 3 000 agentes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
n = 3_000
rng = np.random.default_rng(7)

df = pd.DataFrame({
    "tenure_months"   : rng.integers(1, 48, n),                    # tiempo en la empresa
    "absences_90d"    : rng.integers(0, 6, n),                    # inasistencias recientes
    "shift_type"      : rng.choice(["day", "night"], n, p=[.7, .3]),
    "performance_q"   : rng.choice(["high", "mid", "low"], n, p=[.3, .5, .2]),
    "schedule_adher%" : rng.uniform(70, 105, n).round(1),          # % adherencia al horario
    "last_raise_mths" : rng.integers(0, 24, n),                   # meses desde √∫ltimo aumento
})

# Regla de negocio para la etiqueta (riesgo de absentismo / rotaci√≥n)
prob_quit = (
      0.05
    + 0.04 * (df["absences_90d"] >= 3)
    + 0.06 * (df["performance_q"] == "low")
    + 0.03 * (df["shift_type"] == "night")
    + 0.03 * (df["last_raise_mths"] > 12)
    - 0.02 * (df["tenure_months"] > 24)
).clip(0, 1)

df["churn_risk"] = rng.binomial(1, prob_quit)   # 1 = alto riesgo

# 3. ‚îÄ‚îÄ‚îÄ X (features) y y (target) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X = df.drop(columns="churn_risk")
y = df["churn_risk"]

# 4. ‚îÄ‚îÄ‚îÄ Train / Test (estratificado para balancear la clase) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

# 5. ‚îÄ‚îÄ‚îÄ Pre-procesamiento: categ√≥ricas ‚Üí OHE, num√©ricas ‚Üí escala ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cat_cols = ["shift_type", "performance_q"]
num_cols = ["tenure_months", "absences_90d", "schedule_adher%", "last_raise_mths"]

preproc = ColumnTransformer(
    [("cat", OneHotEncoder(drop="first"), cat_cols),
     ("num", StandardScaler(),           num_cols)]
)

# 6. ‚îÄ‚îÄ‚îÄ Modelo: Random Forest (robusto, interpretabilidad b√°sica) ‚îÄ‚îÄ‚îÄ‚îÄ
rf = RandomForestClassifier(
        n_estimators=400,
        class_weight="balanced",   # compensa clases desbalanceadas
        random_state=42)

pipe = Pipeline([("prep", preproc),
                 ("clf",  rf)])

# 7. ‚îÄ‚îÄ‚îÄ Entrena ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
pipe.fit(X_tr, y_tr)

# 8. ‚îÄ‚îÄ‚îÄ Eval√∫a ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
proba_te = pipe.predict_proba(X_te)[:, 1]
pred_te  = pipe.predict(X_te)

print("ROC-AUC:", roc_auc_score(y_te, proba_te).round(3))
print(classification_report(y_te, pred_te, digits=3))

# 9. ‚îÄ‚îÄ‚îÄ Importancia de variables ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
imp  = pipe.named_steps["clf"].feature_importances_
cols = pipe.named_steps["prep"].get_feature_names_out()
print(pd.Series(imp, index=cols)
        .sort_values(ascending=False)
        .head(10))




"""Paso	Preguntas clave	Artefacto entregable	Ejemplo concreto en un BPO
1. Alinear m√©tricas	¬øQu√© KPI duele m√°s (coste, NPS, SL A)?	Cuadro ‚Äút√©cnico ‚Üí negocio‚Äù	Un MAE de ¬±25 llamadas/h equivale a 2 agentes de sobre-o sub-staffing.
2. Segmentar decisiones	¬øA qui√©n o qu√© proceso afecto?	Regla de decisi√≥n (threshold, ranking)	Si prob_churn ‚â• 0.6 ‚Üí supervisor recibe alerta; 0.4‚Äì0.6 ‚Üí lista de seguimiento.
3. Integrar en el flujo	¬øD√≥nde vive el modelo? ¬øBatch o real-time?	Endpoint REST, SQL view o archivo CSV	Pron√≥stico de volumen se exporta cada tarde al sistema WFM (Aspect, Verint).
4. Automatizar acci√≥n m√≠nima	¬øPuede ejecutarse sin mano humana?	Script, webhook, bot	API reprograma turnos; IVR enruta llamadas largas a agentes senior.
5. Medir el efecto	¬øQu√© var√≠a despu√©s?	Dashboard A/B u ‚Äúantes-despu√©s‚Äù	Tiempos de espera se reducen 15 %; coste de horas extra baja COP 180 M.
6. Cerrar el loop	¬øEl modelo sigue acertando?	Monitorizaci√≥n de drift + retrain	Si MAPE > 8 % durante 3 d√≠as, se dispara retrain semanal."""


"""2 ‚ü∂ Explicar el ‚Äúpor qu√©‚Äù en lenguaje sencillo
2.1 Feature Importance (global)
¬øQu√© es?	Puntuaci√≥n que resume cu√°nto contribuye cada variable al poder predictivo total del modelo.
C√≥mo se calcula	√Årboles: ca√≠da de impureza (Gini/MSE)
Permutaci√≥n: embarajar una columna y medir cu√°nto empeora el error.
C√≥mo contarlo al negocio	‚ÄúSi quit√°ramos la variable absences_90d, nuestro error subir√≠a un 12 %. Por tanto, las ausencias recientes son la palanca m√°s fuerte.‚Äù
Visual t√≠pico	Barras horizontales ordenadas (top-10). Colores verde ‚Üí impacto positivo, rojo ‚Üí negativo (si aplicas SHAP global).

Analog√≠a: ‚ÄúTenemos un equipo de jugadores (features). La importancia global mide cu√°ntos goles marca cada uno en toda la temporada.‚Äù

2.2 SHAP (local + global)
Concepto	Explicaci√≥n para no t√©cnicos
SHAP value	‚ÄúCu√°ntos puntos suma o resta cada variable al resultado final de esta fila concreta comparado con el promedio.‚Äù
Base value	El ‚Äúpromedio de goles‚Äù de un partido hipot√©tico con todos los equipos mezclados.
Propiedad clave	Las contribuciones SIEMPRE suman exactamente la predicci√≥n (transparencia contable).
Visuales	‚Ä¢ Waterfall: barra apilada por una sola observaci√≥n.
‚Ä¢ Beeswarm: nube coloreada (todas las observaciones) para ver direcci√≥n e intensidad.

Met√°fora del pastel
Imagina que la predicci√≥n es un pastel entero (porci√≥n de 550 s AHT). SHAP reparte el pastel en porciones y etiqueta cada trozo con el ingrediente responsable:
+150 s porque es call_type = tech
+80 s porque ivr_steps = 7
-60 s porque agent_tenure = 40 meses
‚Ä¶ hasta completar los 550 s."""

"""5.1 Buenas pr√°cticas de privacidad y √©tica en anal√≠tica
Pilar	Qu√© significa en la pr√°ctica	Herramientas / controles
Minimizaci√≥n	Recoge solo los campos estrictamente necesarios para el caso de uso.	Data-mapping previo, checklist de campos ‚Äúmust-have‚Äù.
Anonimizaci√≥n / seudonimizaci√≥n	Reemplaza identificadores directos (c√©dula, e-mail) por claves internas, y separa la tabla de equivalencias en un vault.	Hash + salt, tokenizaci√≥n, servicios KMS (AWS KMS, Azure Key Vault).
Prop√≥sito claro & consentimiento	Define la finalidad (e.g., ‚Äúmejorar tiempos de espera‚Äù) y comun√≠cala al cliente o a RR. HH.	Notices ‚Äújust-in-time‚Äù, cl√°usulas de consentimiento, opt-out.
Limitaci√≥n de retenci√≥n	Establece ventanas: logs de transcripciones ‚â§ 90 d√≠as, m√©tricas agregadas 2 a√±os.	Pol√≠ticas de borrado autom√°tico (lifecycle rules S3, GCS).
Seguridad y acceso m√≠nimo	Pol√≠ticas IAM basadas en rol; cifrado en reposo y en tr√°nsito.	IAM, VPC, TLS, row-level security.
No maleficencia & equidad	Eval√∫a si el modelo perjudica a grupos protegidos (sexo, edad).	M√©tricas de equidad (disparate impact), auditor√≠as externas.
Transparencia	Documenta l√≥gica del modelo y los datos usados; proporciona explicaciones (SHAP) a usuarios afectados.	Model cards, data sheets, panel FAQ.

Marco r√°pido (5 preguntas para cada nuevo proyecto)

¬øQu√© datos personales requiere?

¬øPodemos cumplir el objetivo con menos datos?

¬øDurante cu√°nto tiempo son necesarios?

¬øQui√©n tendr√° acceso y bajo qu√© rol?

¬øQu√© da√±o potencial hay si el modelo se equivoca o se filtran los datos?

5.2 Manejo de datos incompletos o err√≥neos
Paso	Buen h√°bito	Detalle t√©cnico
1. Perfilado inicial	Usa df.info(), df.describe(), tests de nulls/outliers, data profiling autom√°tico (Great Expectations, Soda).	Distingue entre MCAR, MAR, MNAR para decidir imputaci√≥n.
2. Regla de negocio	Define qu√© es un valor ‚Äúposible‚Äù (ej. AHT < 5 s o > 5 000 s = error).	Cat√°logo de ‚Äúrango v√°lido‚Äù por campo.
3. Depuraci√≥n	Corrige formatos, unidades, fechas imposibles; crea columnas limpias, no sobre-escribe el original.	to_datetime(errors='coerce'), parseadores robustos, casting expl√≠cito.
4. Imputaci√≥n	- Simple: media/mediana para num√©ricos, moda para categ√≥ricas (< 5 % faltante).
- Avanzada: KNNImputer, IterativeImputer (MICE), missForest.	Guarda flag is_imputed para trazabilidad.
5. Revisi√≥n de sesgo	Comprueba que la imputaci√≥n no altere la distribuci√≥n o favorezca a un grupo.	Comparaci√≥n de histogramas antes/despu√©s, pruebas KS.
6. Pipeline reproducible	Automatiza limpieza e imputaci√≥n en sklearn.Pipeline o Spark ETL para evitar c√≥digo ‚Äúen cuaderno suelto‚Äù.	Versiona artefactos en DVC o LakeFS.

5.3 Principios de gobernanza de datos y modelos predictivos
Dominio	Qu√© controlar	Pr√°ctica recomendada
Lineaje	Desde la ingesta hasta el dashboard.	Metadata autom√°tica (OpenLineage, Airflow + Marquez).
Cat√°logo & Glossary	Definici√≥n √∫nica por campo (‚ÄúAHT‚Äù, ‚ÄúAfter-Call Work‚Äù).	Data Catalog (AWS Glue, Azure Purview), Business Glossary.
Versionado de datos	Cambios en esquema y particiones.	Delta Lake / Iceberg con control git-like.
Registro de modelos	Versiones, m√©tricas, costumbre de env√≠o a producci√≥n.	MLflow Model Registry, Evidently AI.
Pol√≠ticas de acceso	Qui√©n puede ver PII, qui√©n puede re-entrenar.	RBAC, columnas enmascaradas, policy-as-code (OPA, Lake Formation).
Auditor√≠a & logging	Query logs, inferencias servidas, drift.	CloudTrail, BigQuery Audit, Prometheus + Grafana.
Ciclo de vida	Caducidad de features y modelos; plan de retrain.	Cron Airflow, triggers por concept drift (> Œî error 8 %)."""

"""1. Conjuntos de Entrenamiento, Validaci√≥n y Test
Conjunto	Qu√© se hace con √©l	Por qu√© es necesario
Entrenamiento (train)	El algoritmo ajusta sus par√°metros internos (Œ≤ de la regresi√≥n, pesos de una red, etc.).	El modelo ‚Äúaprende‚Äù aqu√≠; ver los dem√°s conjuntos producir√≠a fuga de informaci√≥n (data leakage).
Validaci√≥n (val, dev)	Se *sintonizan los hiper-par√°metros (n √°rboles, profundidad, Œª de regularizaci√≥n).	Evita que el modelo se ‚Äúhaga trampa‚Äù optimizando contra el test.
Test (hold-out final)	Se calcula la m√©trica que vas a reportar al negocio o al art√≠culo.	Simula rendimiento en datos futuros nunca vistos. Una sola mirada; no se re-entrena despu√©s.

Regla de oro: todo paso que calcule un n√∫mero (media, varianza, umbral, pesos) s√≥lo se ajusta con train y se reutiliza en val/test.

2. M√©todos de Validaci√≥n
2.1 Hold-out simple
python
Copiar
Editar"""
from sklearn.model_selection import train_test_split
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y,
                                                  test_size=0.3,
                                                  random_state=42,
                                                  stratify=y)  # para clasifs. desequilibradas
X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp,
                                                test_size=0.5,
                                                random_state=42,
                                                stratify=y_tmp)
"""Ventaja: r√°pido; perfecto cuando tienes millones de filas.

Desventaja: un solo corte puede ser poco representativo ‚Üí alta varianza de la m√©trica si el dataset es peque√±o o muy heterog√©neo.

2.2 K-Fold Cross-Validation (CV)
python
Copiar
Editar"""
from sklearn.model_selection import KFold, cross_validate
kf   = KFold(n_splits=5, shuffle=True, random_state=42)
scor = cross_validate(modelo, X, y,
                      cv=kf,
                      scoring=['neg_mean_absolute_error',
                               'r2'],
                      return_train_score=True)
print("MAE CV      :", -scor['test_neg_mean_absolute_error'].mean())
print("MAE std     :",  scor['test_neg_mean_absolute_error'].std())
print("Train vs Val:", -scor['train_neg_mean_absolute_error'].mean())
"""Variaci√≥n	Cu√°ndo usarla	Detalle
StratifiedKFold	Clasificaci√≥n con clases raras.	Mantiene proporciones de cada clase en cada fold.
GroupKFold	Datos con unidades repetidas (cliente, agente).	Ning√∫n ‚Äúgrupo‚Äù puede quedar repartido entre train y val. Evita leakage por identidad.
TimeSeriesSplit	Series temporales.	Respeta el orden cronol√≥gico: los folds son ventanas crecientes de pasado ‚Üí futuro.

Ventaja: usa todos los datos para validar (repetidas veces) ‚Üí menor varianza de la estimaci√≥n final.
Costo: multiplica tiempo de entrenamiento por k.

3. Overfitting, Underfitting y la Curva Sesgo-Varianza
Situaci√≥n	S√≠ntomas	Gr√°fica t√≠pica	Soluciones
Underfitting	Alto error en train y val.	Curvas planas y altas.	Modelo m√°s complejo, m√°s features, transformar variables.
Overfitting	Error muy bajo en train, alto en val/test.	Gran separaci√≥n entre curvas.	Regularizaci√≥n, m√°s datos, limitar complejidad, usar CV/early-stopping.

3.1 Regularizaci√≥n ‚Üí penalizar la complejidad
Modelo	Penalizaci√≥n	Hiper-par√°metro
Ridge (L2)	‚àëŒ≤¬≤	alpha
Lasso (L1)	‚àë	Œ≤
Elastic Net	mezcla L1+L2	alpha, l1_ratio
√Årboles / RF	limitar profundidad, hojas m√≠nimas	max_depth, min_samples_leaf
Boosting (XGBoost, LightGBM)	eta (learning rate), max_depth, subsample	‚Äî
Redes	Dropout, weight decay	tasa dropout, Œª

Ejemplo en 6 l√≠neas: Grid Search + CV
python
Copiar
Editar"""
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
param = {"alpha": np.logspace(-3, 2, 6)}
grid  = GridSearchCV(Ridge(), param, cv=5,
                     scoring="neg_root_mean_squared_error")
grid.fit(X_train, y_train)
print("Œ± √≥ptimo:", grid.best_params_)
print("RMSE CV :", -grid.best_score_)
4. Combinar todo en una Pipeline reproducible
python
Copiar
Editar
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso

pipe = Pipeline([
    ("scale", StandardScaler()),      # se ajusta s√≥lo en train
    ("model", Lasso(alpha=0.1))
])
"""El Pipeline se entrena y valida completo dentro de cada fold ‚Üí garantiza que el escalador no ‚Äúhuela‚Äù datos de validaci√≥n.

En producci√≥n llamas pipe.predict(new_data) y obtienes la misma transformaci√≥n.

5. Preguntas que podr√≠an caerte en la prueba (y c√≥mo responder)
Pregunta del entrevistador	Respuesta ‚Äúde libro‚Äù (‚â§ 30 s)
¬øPor qu√© no usar s√≥lo train y test?	Ajustar hiper-par√°metros mirando el test hace que lo ‚Äúconsumas‚Äù. Necesitas un set aparte o CV.
¬øC√≥mo eliges k en K-Fold?	5 √≥ 10 en datasets grandes; Leave-One-Group-Out si hay pocos grupos; TimeSeriesSplit para tiempos.
¬øQu√© pasa si mi modelo es lento?	Usa ShuffleSplit con 3-4 folds aleatorios o hold-out estratificado; paraleliza CV (n_jobs=-1).
¬øC√≥mo detectas data leakage?	M√©trica inusualmente alta, features con alta correlaci√≥n con el target obtenida post-evento (e.g. fecha_cierre al predecir duraci√≥n). Evito esto en Pipeline.
¬øRegularizaci√≥n siempre mejora?	Reduce varianza a costa de algo de sesgo. Si ya est√°s under-fitting, har√° el error mayor; observo las curvas."""

"""1. Separar datos y entrenar el modelo m√°s simple (baseline)
Qu√© demuestra	C√≥digo
Comprensi√≥n de la API ¬´fit / predict¬ª y de la m√©trica adecuada	python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\nprint(\"Acc:\", accuracy_score(y_test, clf.predict(X_test)))\n

2. Preprocesamiento + modelo en una Pipeline (evitar leakage)
Qu√© demuestra	C√≥digo
Uso de StandardScaler, buenas pr√°cticas de ML productivo	python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n (\"scaler\", StandardScaler()),\n (\"model\", LogisticRegression(max_iter=1000))\n])\npipe.fit(X_train, y_train)\n

3. ColumnTransformer para datos mixtos (num√©ricos + categ√≥ricos)
Qu√© demuestra	C√≥digo
Manejo de features heterog√©neas y transformaciones espec√≠ficas	python\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import Ridge\n\n# Ejemplo artificial\ndf = pd.DataFrame({\n \"age\": [23, 45, 31, 35],\n \"income\": [40_000, 90_000, 60_000, 70_000],\n \"city\": [\"Bogot√°\", \"Medell√≠n\", \"Cali\", \"Bogot√°\"],\n \"spend\": [300, 800, 400, 560]\n})\nX = df.drop(\"spend\", axis=1)\ny = df[\"spend\"]\n\nnumeric = [\"age\", \"income\"]\ncategorical = [\"city\"]\n\npre = ColumnTransformer([\n (\"num\", StandardScaler(), numeric),\n (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical)\n])\n\nmodel = Pipeline([\n (\"pre\", pre),\n (\"reg\", Ridge(alpha=1.0))\n])\nmodel.fit(X, y)\n

4. Cross-validation r√°pida con cross_val_score
Qu√© demuestra	C√≥digo
Comprensi√≥n de varianza/overfitting y reproducibilidad (random_state)	python\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nscores = cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\")\nprint(\"CV mean ¬± std:\", np.mean(scores), \"+/-\", np.std(scores))\n

5. GridSearchCV (b√∫squeda de hiperpar√°metros)
Qu√© demuestra	C√≥digo
Capacidad de exprimir rendimiento sin data leakage	python\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n \"model__C\": [0.1, 1, 10],\n \"model__penalty\": [\"l2\"],\n}\n\ngs = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\ngs.fit(X, y)\nprint(\"Best score:\", gs.best_score_)\nprint(\"Best params:\", gs.best_params_)\n

6. M√©tricas de clasificaci√≥n avanzadas
Qu√© demuestra	C√≥digo
Conocer cu√°ndo Accuracy no basta, y saber generar informes	python\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, pipe.predict(X_test)))\nprint(classification_report(y_test, pipe.predict(X_test)))\n

7. Guardar y cargar modelos (joblib)
Qu√© demuestra	C√≥digo
Reproducibilidad y deployment b√°sico	python\nimport joblib\n\njoblib.dump(pipe, \"model.pkl\")\nmodel_loaded = joblib.load(\"model.pkl\")\nprint(model_loaded.predict(X_test[:3]))\n

8. Aprendizaje incremental con partial_fit
Qu√© demuestra	C√≥digo
Manejo de streams / big data y del formato partial_fit	python\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = load_digits(return_X_y=True)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nsgd = SGDClassifier(loss=\"log_loss\", random_state=42)\nclasses = np.unique(y)\n\nfor i in range(0, len(X), 256): # mini-batches\n sgd.partial_fit(X[i:i+256], y[i:i+256], classes=classes)\n"""

1. L√≠nea b√°sica con t√≠tulo y ejes
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)

plt.figure()                         # nueva figura
plt.plot(x, y, linewidth=2)
plt.title("Seno simple")
plt.xlabel("x")
plt.ylabel("sin(x)")
plt.grid(True)
plt.show()
2. Varias l√≠neas y leyenda
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
plt.figure()
plt.plot(x, np.sin(x), label="sin(x)")
plt.plot(x, np.cos(x), linestyle="--", label="cos(x)")
plt.legend()
plt.tight_layout()
plt.show()
3. Dispersi√≥n con mapeo de colores y barra de color
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)
x = np.random.randn(200)
y = np.random.randn(200)
colors = np.sqrt(x**2 + y**2)        # magnitud

plt.figure()
sc = plt.scatter(x, y, c=colors, alpha=0.8)
plt.colorbar(sc, label="Magnitud")
plt.title("Scatter coloreado por magnitud")
plt.show()
4. Histograma + curva de densidad (kernel)
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

data = np.random.normal(loc=0, scale=1, size=1000)

plt.figure()
plt.hist(data, bins=30, density=True, alpha=0.5)

kde = gaussian_kde(data)
x = np.linspace(data.min(), data.max(), 200)
plt.plot(x, kde(x), linewidth=2)
plt.title("Histograma con KDE")
plt.show()
5. Mosaico de subplots
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

fig, axes = plt.subplots(2, 2, figsize=(8, 6))

x = np.linspace(0, 2*np.pi, 100)
axes[0, 0].plot(x, np.sin(x));  axes[0, 0].set_title("sin")
axes[0, 1].plot(x, np.cos(x));  axes[0, 1].set_title("cos")
axes[1, 0].plot(x, np.tan(x));  axes[1, 0].set_title("tan")
axes[1, 1].plot(x, np.exp(x));  axes[1, 1].set_title("exp")

fig.tight_layout()
plt.show()
6. Barras con barras de error
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

labels = ["A", "B", "C", "D"]
values = [20, 34, 30, 35]
errors = [2, 3, 4, 1]

plt.figure()
plt.bar(labels, values, yerr=errors, capsize=5)
plt.ylabel("Puntuaci√≥n")
plt.title("Medias ¬± error est√°ndar")
plt.show()
7. Boxplot por grupo categ√≥rico
python
Copiar
Editar
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(0)
group1 = np.random.normal(50, 10, size=200)
group2 = np.random.normal(55, 15, size=200)

plt.figure()
plt.boxplot([group1, group2], labels=["Grupo 1", "Grupo 2"])
plt.ylabel("Valor")
plt.title("Comparaci√≥n de distribuciones")
plt.show()
8. Serie temporal con anotaci√≥n
python
Copiar
Editar
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

dates = pd.date_range("2025-01-01", periods=180)
values = np.cumsum(np.random.randn(180))  # paseo aleatorio

plt.figure(figsize=(10, 4))
plt.plot(dates, values)
plt.title("Serie temporal simulada")
plt.axvline(pd.Timestamp("2025-04-01"), linestyle="--")
plt.annotate("Evento clave", xy=("2025-04-01", values[90]),
             xytext=("2025-02-15", values.max()),
             arrowprops=dict(arrowstyle="->"))
plt.tight_layout()
plt.show()

"""1. ¬øQu√© son los hiperpar√°metros?
Par√°metros ‚Üî se aprenden durante el entrenamiento (Œ≤ en regresi√≥n, pesos de redes).

Hiperpar√°metros ‚Üî decides antes de entrenar: profundidad de √°rbol, alpha de Lasso, n√∫mero de vecinos en K-NN, tasa de aprendizaje en boosting, etc.

Influyen directamente en sesgo y varianza ‚áí rendimiento.

Para encontrar la combinaci√≥n √≥ptima se recorre un espacio de b√∫squeda y se mide la m√©trica (MAE, ROC-AUC‚Ä¶) mediante validaci√≥n cruzada.

2. Grid Search vs Random Search
Caracter√≠stica	Grid Search	Random Search
C√≥mo funciona	Prueba todas las combinaciones de la rejilla.	Muestra combinaciones aleatorias de distribuciones dadas.
Ventaja	Garantiza evaluar cada punto definido.	Cubre espacios grandes con muchos menos experimentos; encuentra buenas regiones r√°pido.
Cu√°ndo usar	Pocos hiperpar√°metros y/o valores discretos (K-NN: k = [3,5,7,9]).	Rangos amplios o continuos (Random Forest: n_estimators 50-500; max_depth 3-30).
Costo	Crece exponencialmente; puede volverse inabordable.	Proporcional al n¬∫ de iteraciones que fijes.

Regla pr√°ctica: Grid para ‚â§ 3 hiperpar√°metros con ~10 √ó 10 √ó 5 puntos; Random (o t√©cnicas bayesianas) para todo lo dem√°s.

3. Ejemplo 1 ‚Äî Grid Search con Ridge (regresi√≥n)
python
Copiar
Editar
import numpy as np, pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# X, y ya preparados ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
cv       = KFold(n_splits=5, shuffle=True, random_state=42)
pipeline = make_pipeline(StandardScaler(), Ridge())

param_grid = {
    "ridge__alpha": np.logspace(-3, 2, 6)     # 0.001 ‚Üí 100
}

gcv = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        scoring="neg_mean_absolute_error",
        cv=cv,
        n_jobs=-1)      # usa todos los n√∫cleos

gcv.fit(X, y)

print("Mejor Œ± :", gcv.best_params_["ridge__alpha"])
print("MAE CV  :", -gcv.best_score_)
L√©elo as√≠

ridge__alpha ‚Üí prefijo ridge porque el modelo est√° dentro del Pipeline.

logspace explora 6 potencias de 10; GridSearchCV probar√° 6 modelos √ó 5 folds = 30 entrenamientos.

best_params_ y best_estimator_ quedan listos para predict() sobre test o producci√≥n.

4. Ejemplo 2 ‚Äî Random Search con Random Forest (clasificaci√≥n)
python
Copiar
Editar
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold

rf = RandomForestClassifier(class_weight="balanced", random_state=42)

param_dist = {
    "n_estimators" : np.arange(100, 801, 50),     # 100‚Ä¶800
    "max_depth"    : np.arange(4, 31),            # 4‚Ä¶30
    "min_samples_leaf": [1, 2, 4, 6],
    "max_features" : ["sqrt", "log2", 0.5]        # n_features**0.5, log2, 50 %
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rs = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_dist,
        n_iter=60,                 # 60 combinaciones aleatorias
        scoring="roc_auc",
        cv=cv,
        n_jobs=-1,
        random_state=42)

rs.fit(X, y)

print("Mejores hiperpar√°metros:", rs.best_params_)
print("ROC-AUC promedio CV   :", rs.best_score_)
Por qu√© funciona
Aun con un espacio de ‚âà 28 k combinaciones posibles, s√≥lo evaluamos 60 ‚áí gran ahorro.

Para la mayor√≠a de modelos, las primeras 50-100 pruebas aleatorias capturan > 90 % del beneficio, porque muchas combinaciones malas son descartadas.

5. Buenas pr√°cticas
Pre-procesamiento dentro de Pipeline
Escaladores, imputadores, codificadores‚Ä¶ as√≠ cada fold se ajusta s√≥lo con datos de train (sin leakage).

Escoge la m√©trica correcta

Regresi√≥n: MAE, RMSE, R¬≤.

Clasificaci√≥n desequilibrada: ROC-AUC, F1, PR-AUC.

Fija random_state
Para tener reproducibilidad en Random Search y modelos estoc√°sticos.

n_jobs = -1
Aprovecha todos los n√∫cleos de la m√°quina; Grid Search paralelo mitiga el coste.

Validaci√≥n temporal
En series de tiempo, usa TimeSeriesSplit dentro de Grid/Random Search.

Salvaguarda contra overfitting
Despu√©s de encontrar los hiperpar√°metros √≥ptimos, re-entrena el modelo final en (train + val) y eval√∫alo en test.

6. Pasos resumidos para tu prueba
Definir el espacio

Discreto ‚Üí lista explicitada (Grid).

Continuo ‚Üí rango + distribuci√≥n (loguniform, randint).

Elegir m√©todo y CV
GridSearchCV o RandomizedSearchCV + KFold / StratifiedKFold / TimeSeriesSplit.

Lanzar b√∫squeda
fit() ejecuta el ciclo:
for hp_comb in space: for fold in cv: train ‚Üí val ‚Üí m√©trica

Inspeccionar resultados
cv_results_ contiene cada combinaci√≥n, puedes graficar hiperpar√°metro ‚Üî score con pandas/matplotlib.

Exportar el mejor modelo
Serializa con joblib.dump(rs.best_estimator_, 'rf_best.pkl').

Con estas nociones y ejemplos podr√°s justificar qu√© m√©todo usar, c√≥mo configurarlo y c√≥mo interpretar los resultados en cualquier entrevista o prueba t√©cnica. ¬°A practicar!"""

"""Abrev.	Significado / Descripci√≥n breve
A/B	A/B Testing ‚Üí experimento con 2 variantes (control A, tratamiento B).
AIC	Akaike Information Criterion ‚Üí m√©trica para comparar modelos (penaliza complejidad).
ANN	Artificial Neural Network ‚Üí red neuronal artificial.
API	Application Programming Interface ‚Üí punto de acceso programable.
AR / MA / ARMA / ARIMA / SARIMA	AutoRegressive, Moving Average, combinaciones para series temporales; SARIMA a√±ade estacionalidad.
AUC / ROC-AUC	Area Under the (ROC) Curve ‚Üí probabilidad de rankeo correcto entre clases.
AWS	Amazon Web Services (plataforma cloud).
BIC	Bayesian Information Criterion ‚Üí similar a AIC con mayor penalizaci√≥n.
BERT	Bidirectional Encoder Representations from Transformers (modelo NLP).
BI	Business Intelligence.
CI/CD	Continuous Integration / Continuous Deployment (pipeline de entrega).
CNN	Convolutional Neural Network (visi√≥n por computadora).
CV	Cross-Validation ‚Üí validaci√≥n cruzada.
DAG	Directed Acyclic Graph ‚Üí flujo de tareas (Airflow, etc.).
DBMS	Database Management System.
DL	Deep Learning.
EDA	Exploratory Data Analysis ‚Üí an√°lisis exploratorio.
ELT / ETL	Extract-Load-Transform / Extract-Transform-Load (ingesta de datos).
ELO	Expected Log-likelihood Optimisation (o rating de habilidad; contexto depende).
EM / EMR	Expectation-Maximization / Elastic MapReduce (AWS).
ETS	Error-Trend-Seasonal (modelo Holt-Winters).
F1	Harmonic mean de precisi√≥n y recall.
FN / FP / TP / TN	False Negative / False Positive / True Positive / True Negative.
GAN	Generative Adversarial Network.
GDPR	General Data Protection Regulation (UE).
GPU	Graphics Processing Unit.
GRU	Gated Recurrent Unit (variante RNN).
GS / RS	Grid Search / Random Search (b√∫squeda de hiper-par√°metros).
IAM	Identity and Access Management (control de permisos).
ICA	Independent Component Analysis (reducci√≥n de dimensionalidad).
K-Fold	Validaci√≥n cruzada en k particiones.
KNN	K-Nearest Neighbors.
KS-test	Kolmogorov‚ÄìSmirnov test (comparaci√≥n de distribuciones).
L1 / L2	Normas de regularizaci√≥n (Lasso / Ridge).
LDA	Linear Discriminant Analysis (clasificaci√≥n) o Latent Dirichlet Allocation (t√≥picos).
LIME	Local Interpretable Model-agnostic Explanations.
LOOCV	Leave-One-Out Cross-Validation.
LSTM	Long Short-Term Memory (RNN avanzada).
MAE / MAPE / MASE	Mean Absolute Error / Percentage Error / Scaled Error.
MLE	Maximum Likelihood Estimation.
MLOps	Operaciones de Machine Learning (ciclo de vida en producci√≥n).
MNAR / MAR / MCAR	Tipos de datos faltantes: Missing Not At Random / At Random / Completely At Random.
MSE / RMSE	Mean Squared Error / Root MSE.
NLP	Natural Language Processing.
OLS / GLS	Ordinary / Generalized Least Squares (regresi√≥n).
PCA	Principal Component Analysis.
PII	Personally Identifiable Information.
PR-AUC	√Årea bajo la curva Precision-Recall.
R¬≤ / R-adj¬≤	Coefficient of Determination y su versi√≥n ajustada.
RBF	Radial Basis Function kernel (SVM).
RFE	Recursive Feature Elimination (selecci√≥n de variables).
RF	Random Forest.
RNN	Recurrent Neural Network.
ROC	Receiver Operating Characteristic curve.
SHAP	SHapley Additive exPlanations (interpretabilidad).
SMOTE	Synthetic Minority Over-sampling Technique (balanceo de clases).
SQL / NoSQL	Structured Query Language / bases de datos ‚Äúno s√≥lo SQL‚Äù.
SSE / SSR / SST	Sum of Squared Errors / Residuals / Total (ANOVA).
SVM / SVR	Support Vector Machine / Regression.
t-SNE / UMAP	T√©cnicas de proyecci√≥n no lineal para visualizaci√≥n.
TF-IDF	Term Frequency‚ÄìInverse Document Frequency (ponderaci√≥n de textos).
TPOT / Optuna / Hyperopt	Librer√≠as de AutoML / optimizaci√≥n bayesiana.
TSF / TSS	Time Series Forecasting / Split.
VIF	Variance Inflation Factor (multicolinealidad).
XGB	XGBoost (extreme gradient boosting)."""

"""5 modelos de predicci√≥n ‚Äúimprescindibles‚Äù en un call center
(Qu√© resuelven, por qu√© se prefieren y un micro-ejemplo para que veas la idea en pr√°ctica)

#	Modelo<br>(familia)	Caso de uso t√≠pico	¬øPor qu√© encaja bien?	Ejemplo de 2-3 l√≠neas de c√≥digo
1	SARIMA / Prophet
(series temporales)	Pron√≥stico de volumen de llamadas por franja de 15 min o 1 h.	Capturan estacionalidad semanal y picos de campa√±as sin requerir gran volumen de features.	python<br>from prophet import Prophet<br>df_prophet = df[['ds','y']] # fecha, volumen<br>m = Prophet(weekly_seasonality=4).fit(df_prophet)<br>forecast = m.predict(future_df)
2	Gradient Boosting Regressor
(XGBoost / LightGBM)	Predicci√≥n de AHT (duraci√≥n) y After-Call Work combinando: tipo de llamada, IVR steps, experiencia del agente, tier del cliente.	Maneja no linealidad, interacciones y outliers; se entrena r√°pido y da importancias robustas.	python<br>import lightgbm as lgb<br>lgb_reg = lgb.LGBMRegressor(max_depth=8, n_estimators=400)<br>lgb_reg.fit(X_train, y_train)
3	Random Forest Classifier	Absentismo / rotaci√≥n de agentes (clasificaci√≥n 0-1: se ausentar√°/renunciar√°).	Pocos supuestos estad√≠sticos, interpreta ‚Äúfeature importance‚Äù, soporta mezcla num√©rico-categ√≥rica y desbalance.	python<br>from sklearn.ensemble import RandomForestClassifier<br>rf = RandomForestClassifier(n_estimators=500, class_weight='balanced').fit(X,y)
4	Logistic Regression
(baseline penalizado L1/L2)	Probabilidad de conversi√≥n en campa√±as outbound o de que un cliente acepte venta cruzada.	F√°cil de explicar (‚Äúodds ratio‚Äù), r√°pido de entrenar, sirve de benchmark antes de usar modelos complejos.	python<br>from sklearn.linear_model import LogisticRegression<br>logit = LogisticRegression(penalty='l2', C=1).fit(X,y)
5	BERT-finetune / Sentiment Model
(NLP Transformer)	Predicci√≥n de CSAT a partir de transcripci√≥n: estima la satisfacci√≥n antes de la encuesta y alerta casos cr√≠ticos."""

üì¶ Carga y exploraci√≥n b√°sica de datos

import pandas as pd

# Cargar CSV
df = pd.read_csv('archivo.csv')

# Ver las primeras y √∫ltimas filas
df.head()           # Primeras 5 filas
df.tail(3)          # √öltimas 3 filas

# Tama√±o
df.shape            # (filas, columnas)

# Nombres de columnas
df.columns.tolist()

# Tipos de datos por columna
df.dtypes

# Resumen estad√≠stico
df.describe()

# Informaci√≥n general
df.info()

# Valores √∫nicos
df['columna'].unique()

# Conteo de valores √∫nicos
df['columna'].value_counts()
üîç Filtrado y selecci√≥n de datos

# Selecci√≥n por nombre de columna
df['columna']
df[['col1', 'col2']]

# Filtrado condicional
df[df['edad'] > 30]
df[(df['sexo'] == 'F') & (df['ciudad'] == 'Bogot√°')]

# Filtrado por valores faltantes
df[df['columna'].isnull()]
df[df['columna'].notnull()]
‚úçÔ∏è Creaci√≥n y modificaci√≥n de columnas

# Crear nueva columna
df['edad_doble'] = df['edad'] * 2

# Modificar valores con funci√≥n
df['nombre_mayus'] = df['nombre'].str.upper()

# Reemplazar valores
df['sexo'] = df['sexo'].replace({'M': 'Masculino', 'F': 'Femenino'})

# Aplicar funci√≥n fila por fila
df['categoria'] = df.apply(lambda x: 'Mayor' if x['edad'] > 60 else 'Adulto', axis=1)
üßº Limpieza de datos

# Eliminar columnas o filas
df.drop(columns=['col1', 'col2'], inplace=True)
df.drop(index=[0, 1], inplace=True)

# Renombrar columnas
df.rename(columns={'antiguo': 'nuevo'}, inplace=True)

# Cambiar tipo de dato
df['fecha'] = pd.to_datetime(df['fecha'])

# Rellenar valores faltantes
df['salario'].fillna(0, inplace=True)

# Eliminar duplicados
df.drop_duplicates(inplace=True)

# Resetear √≠ndice
df.reset_index(drop=True, inplace=True)
üìä Agrupamiento y resumen

# Agrupar y resumir
df.groupby('ciudad')['ventas'].mean()
df.groupby(['ciudad', 'producto'])['ventas'].sum().reset_index()

# Contar elementos
df.groupby('producto').size()

# Agregaciones m√∫ltiples
df.groupby('ciudad')['ventas'].agg(['mean', 'sum', 'count'])
üîÄ Ordenar y combinar datos

# Ordenar por columnas
df.sort_values(by='ventas', ascending=False)

# Unir dataframes (merge estilo SQL)
df_merged = pd.merge(df1, df2, on='id', how='left')

# Concatenar (apilar) dataframes
df_concat = pd.concat([df1, df2], axis=0)   # vertical
üß≠ Indexaci√≥n avanzada

# Establecer columna como √≠ndice
df.set_index('fecha', inplace=True)

# Selecci√≥n por rango de √≠ndice
df.loc['2023-01-01':'2023-01-31']

# Resetear √≠ndice a n√∫mero entero
df.reset_index(inplace=True)
üìå Extra: m√©todos √∫tiles para pruebas

# Mapear valores
df['tipo'] = df['codigo'].map({1: 'A', 2: 'B'})

# Quitar espacios en texto
df['cliente'] = df['cliente'].str.strip()

# Ver nulos por columna
df.isnull().sum()

# Filtrar por m√∫ltiples valores
df[df['ciudad'].isin(['Bogot√°', 'Medell√≠n'])]
‚úÖ Checklist para la prueba
Tarea com√∫n	M√©todo clave
Leer datos	read_csv()
Explorar	head(), info(), describe()
Limpiar	drop(), fillna(), replace(), drop_duplicates()
Crear columnas	apply(), map(), str.upper()
Agrupar	groupby(), agg()
Filtrar	df[...], isin(), isnull()
Unir tablas	merge(), concat()
Ordenar	sort_values()

1. Variables, tipos y conversi√≥n

# Asignaci√≥n
x = 10            # int
pi = 3.1416       # float
nombre = "Miguel" # str
es_mayor = True   # bool

# Conversi√≥n expl√≠cita
altura = "1.75"
altura = float(altura)   # str ‚Üí float

# Ver tipo
type(altura)  # <class 'float'>
2. Operadores b√°sicos
Tipo	Ejemplo	Resultado
Aritm√©ticos	5 ** 2, 7 % 3	25, 1
Comparaci√≥n	10 >= 8	True
L√≥gicos	a and b, not x	‚Äî
Asignaci√≥n compuesta	x += 1	x = x + 1
Membership	'mig' in 'miguel'	True

3. Estructuras de datos fundamentales

# Lista (mutable)
nums = [1, 2, 3]
nums.append(4)

# Tupla (inmutable)
coords = (4, 5)

# Conjunto (√∫nicos, sin orden)
vocales = {'a', 'e', 'i', 'o', 'u'}
vocales.add('y')

# Diccionario (clave-valor)
persona = {'nombre': 'Ana', 'edad': 30}
persona['edad'] += 1
4. Slicing y unpacking

lista = ['a','b','c','d','e']
lista[1:4]      # ['b','c','d']
lista[::-1]     # ['e','d','c','b','a']  (reversa)

a, b, *resto = lista
# a='a', b='b', resto=['c','d','e']
5. Control de flujo

# Condicional
if edad >= 18:
    print("Adulto")
elif edad >= 13:
    print("Adolescente")
else:
    print("Ni√±o")

# Bucle for
for i in range(1, 4):
    print(i)   # 1 2 3

# Bucle while
contador = 0
while contador < 3:
    contador += 1
6. Funciones y lambda

def area_rect(base, altura=1):
    """Devuelve √°rea de un rect√°ngulo."""
    return base * altura

area_rect(4, 3)      # 12
area_rect(5)         # 5 (usa altura por defecto)

# Funci√≥n an√≥nima
al_cuadrado = lambda x: x**2
7. Comprensiones

# Lista de cuadrados
cuadrados = [n**2 for n in range(5)]          # [0,1,4,9,16]

# Dict de pares
pares = {n: n%2==0 for n in range(5)}         # {0:True,1:False,...}
8. Manejo de excepciones

try:
    resultado = 10 / 0
except ZeroDivisionError:
    resultado = None
finally:
    print("Siempre se ejecuta")
9. Archivos

with open('datos.txt', 'w') as f:
    f.write('Hola\n')

with open('datos.txt') as f:
    texto = f.read()
10. M√≥dulos y import

import math
math.sqrt(16)      # 4.0

from statistics import mean
mean([1,2,3])      # 2
11. Clases y objetos (POO)

class Persona:
    def __init__(self, nombre):
        self.nombre = nombre

    def saluda(self):
        return f"Hola, soy {self.nombre}"

p = Persona("Laura")
p.saluda()          # "Hola, soy Laura"
12. Generadores y yield

def cuenta_regresiva(n):
    while n:
        yield n
        n -= 1

for num in cuenta_regresiva(3):   # 3 2 1
    print(num)
13. Decoradores (concepto clave)

def loguea(func):
    def wrapper(*args, **kwargs):
        print("Llamando:", func.__name__)
        return func(*args, **kwargs)
    return wrapper

@loguea
def suma(a, b):
    return a + b

suma(3, 4)   # imprime "Llamando: suma" ‚Üí 7
14. enumerate, zip, map, filter

# enumerate
for idx, letra in enumerate(['a','b','c']):
    print(idx, letra)

# zip
for a, b in zip([1,2], ['uno','dos']):
    print(a, b)

# map / filter
dobles = list(map(lambda x: x*2, [1,2,3]))
pares = list(filter(lambda x: x%2==0, range(6)))
"""Checklist de conceptos b√°sicos
Categor√≠a	M√©todos / palabras clave
Estructuras	list, dict, set, tuple, slicing
Control	if, for, while, range, comprensi√≥n
Funciones	def, lambda, *args, **kwargs
OOP	class, __init__, self, herencia
Librer√≠as	import, from ... import ...
Utilidades	enumerate, zip, map, filter
Robustez	try / except / finally, with open()
Avanzado	yield (generadores), decoradores

üóÑÔ∏è Gu√≠a-express de conceptos b√°sicos de SQL
(Ejemplo + explicaci√≥n en una sola l√≠nea para cada comando, ideal para un repaso antes de una prueba t√©cnica)

Sup√≥n la tabla calls(id, agent_id, call_type, duration, started_at, city, revenue) y la tabla agents(agent_id, name, hire_date, team).

Categor√≠a	Sintaxis m√≠nima	Ejemplo & mini-explicaci√≥n
Seleccionar columnas	SELECT col1, col2 FROM tabla;	SELECT id, duration FROM calls; ‚Üí muestra solo id y duraci√≥n.
Alias	SELECT col AS alias	SELECT duration AS aht FROM calls; renombra duration.
Filtrar filas	WHERE condici√≥n	SELECT * FROM calls WHERE duration > 300; ‚Üí > 5 min.
AND / OR / NOT	cond1 AND cond2	WHERE city='Bogot√°' AND call_type='tech'.
Ordenar	`ORDER BY col [ASC	DESC]`
LIMIT	LIMIT n [OFFSET m]	LIMIT 10; trae 10 filas.
DISTINCT	SELECT DISTINCT col	SELECT DISTINCT city FROM calls; lista ciudades √∫nicas.
Funciones de agregaci√≥n	COUNT, SUM, AVG, MAX, MIN	SELECT AVG(duration) FROM calls; = AHT global.
GROUP BY	GROUP BY col	SELECT city, COUNT(*) FROM calls GROUP BY city; llamadas por ciudad.
HAVING	filtro sobre agregados	HAVING COUNT(*) > 100; (solo ciudades con > 100 llamadas).
JOIN (INNER)	FROM A JOIN B ON cond	SELECT a.id, b.team FROM calls a JOIN agents b ON a.agent_id=b.agent_id;
LEFT / RIGHT JOIN	LEFT JOIN conserva filas de la izquierda	LEFT JOIN agents incluye llamadas sin agente emparejado.
Self-join	alias tabla	SELECT a.id, b.id FROM calls a, calls b WHERE a.agent_id=b.agent_id AND a.id<b.id;
UNION / UNION ALL	concatena resultados	SELECT city FROM intl_calls UNION SELECT city FROM calls;
Subconsulta	(SELECT ‚Ä¶)	SELECT * FROM calls WHERE agent_id IN (SELECT agent_id FROM agents WHERE team='A');
CTE (WITH)	WITH tmp AS (‚Ä¶) SELECT ‚Ä¶	sql WITH long_calls AS (SELECT * FROM calls WHERE duration>300) SELECT COUNT(*) FROM long_calls;
INSERT	INSERT INTO tabla (col1, col2) VALUES (v1, v2);	INSERT INTO agents(name, team) VALUES ('Ana','A');
UPDATE	UPDATE tabla SET col=v WHERE cond;	UPDATE calls SET city='Bogot√°' WHERE id=5;
DELETE	DELETE FROM tabla WHERE cond;	DELETE FROM calls WHERE duration<5;
CREATE TABLE	definici√≥n esquem√°tica	sql CREATE TABLE teams(id SERIAL PRIMARY KEY, name TEXT UNIQUE);
PRIMARY KEY	clave √∫nica + NOT NULL	id SERIAL PRIMARY KEY.
FOREIGN KEY	referencia otra tabla	agent_id INT REFERENCES agents(agent_id).
DEFAULT	valor por defecto	created_at TIMESTAMP DEFAULT NOW().
CHECK	restricci√≥n	CHECK (duration > 0).
INDEX	acelerar b√∫squeda	CREATE INDEX idx_calls_city ON calls(city);
VIEW	consulta guardada	CREATE VIEW aht_by_type AS SELECT call_type, AVG(duration) aht FROM calls GROUP BY call_type;
Window functions	OVER()	SELECT id, duration, AVG(duration) OVER(PARTITION BY call_type) AS avg_type FROM calls;
CASE	l√≥gica condicional	SELECT id, CASE WHEN duration>600 THEN 'larga' ELSE 'corta' END AS categoria FROM calls;
COALESCE / NULL	reemplazar nulos	SELECT COALESCE(city,'(desconocido)') FROM calls;
DATE functions	DATE_TRUNC, EXTRACT	SELECT DATE_TRUNC('month', started_at) mes, COUNT(*) FROM calls GROUP BY mes;
CAST	cambiar tipo	SELECT CAST(duration AS FLOAT)/60 AS minutos FROM calls;
Transactions	BEGIN ‚Ä¶ COMMIT / ROLLBACK	asegura atomicidad de cambios.
Privileges	GRANT SELECT ON tabla TO user;	control de acceso."""

"""‚ë† Crear un Stored Procedure (SP) en MySQL
(obtiene el AHT diario de un rango de fechas para una campa√±a dada)

sql
Copiar
Editar
-- cambiamos el delimitador para que el ; interno no cierre el bloque
DELIMITER $$     

CREATE PROCEDURE sp_aht_diario (
    IN  p_campaign_id INT,          -- par√°metro de entrada
    IN  p_fecha_ini   DATE,
    IN  p_fecha_fin   DATE
)
BEGIN
    /* 1)  agrupamos por fecha;    2)  calculamos la duraci√≥n promedio   */
    SELECT
        DATE(call_date)      AS fecha_llamada,
        ROUND(AVG(duration)) AS aht_segundos
    FROM tb_llamadas
    WHERE campaign_id = p_campaign_id
      AND call_date   BETWEEN p_fecha_ini AND p_fecha_fin
    GROUP BY DATE(call_date)
    ORDER BY fecha_llamada;
END$$

DELIMITER ;      -- restauramos el delimitador normal
C√≥mo leerlo

DELIMITER $$‚ÄÉ‚Üí evita que el ; dentro del cuerpo cierre el CREATE PROCEDURE.

IN‚ÄÉ‚Üí par√°metros de entrada (podr√≠as usar OUT o INOUT si quisieras devolver valores escalares).

BEGIN ‚Ä¶ END‚ÄÉ‚Üí bloque donde escribes SQL normal; admite variables, cursores, IF, WHILE.

GROUP BY DATE(call_date)‚ÄÉ‚Üí calcula el Average Handle Time por d√≠a.

Ejecuci√≥n:

sql
Copiar
Editar
CALL sp_aht_diario(12, '2025-06-01', '2025-06-30');
‚ë° Ejemplo de ROW_NUMBER() (MySQL 8+/PostgreSQL/SQL Server)
sql
Copiar
Editar
/* Objetivo: quedarnos con la llamada m√°s larga de cada agente en el d√≠a */

WITH llamadas_rank AS (
    SELECT
        id,
        agent_id,
        DATE(call_date)   AS dia,
        duration,
        ROW_NUMBER() OVER (
            PARTITION BY agent_id, DATE(call_date)      -- reinicia por agente y d√≠a
            ORDER BY duration DESC                       -- mayor duraci√≥n = fila 1
        ) AS rn
    FROM tb_llamadas
)
SELECT *
FROM llamadas_rank
WHERE rn = 1;          -- s√≥lo la fila top de cada partici√≥n
Puntos clave

ROW_NUMBER() asigna un contador incremental dentro de cada partici√≥n (PARTITION BY).

El ORDER BY dentro de la ventana dicta qu√© fila recibe rn = 1.

El CTE (WITH) hace el resultado legible; podr√≠as usar una subconsulta si tu motor no soporta CTEs.

‚ë¢ SP que usa ROW_NUMBER internamente (MySQL 8+)
sql
Copiar
Editar
DELIMITER $$

CREATE PROCEDURE sp_top_llamadas (
    IN  p_n INT,
    IN  p_fecha DATE
)
BEGIN
    WITH ranked AS (
        SELECT
            id,
            agent_id,
            duration,
            ROW_NUMBER() OVER (PARTITION BY agent_id ORDER BY duration DESC) AS rk
        FROM tb_llamadas
        WHERE DATE(call_date) = p_fecha
    )
    SELECT *
    FROM ranked
    WHERE rk <= p_n                -- top-N por agente
    ORDER BY agent_id, rk;
END$$

DELIMITER ;
p_n define cu√°ntas llamadas ‚Äúm√°s largas‚Äù devolver por agente para un d√≠a concreto.

‚ë£ SP con par√°metro OUT (devuelve una m√©trica scalar)
sql
Copiar
Editar
DELIMITER $$

CREATE PROCEDURE sp_ultimo_id_llamada (OUT p_last_id INT)
BEGIN
    SELECT MAX(id) INTO p_last_id
    FROM tb_llamadas;
END$$

DELIMITER ;

-- uso
CALL sp_ultimo_id_llamada(@ult);
SELECT @ult;    -- muestra el valor capturado en la variable de sesi√≥n"""

"""M√©tricas de error para regresi√≥n: MAE y MSE
Las dos m√©tricas cuantifican la diferencia entre los valores predichos (≈∑) y los reales (y), pero lo hacen de manera distinta, lo que cambia su interpretaci√≥n y las decisiones que tomar√°s con ellas.

1. MAE ‚Äî Mean Absolute Error (Error Absoluto Medio)
Aspecto	Detalle
F√≥rmula	[
\text{MAE}= \frac{1}{n}\sum_{i=1}^{n}\left	y_i-\hat y_i \right
]	
Unidades	Las mismas que la variable objetivo (segundos, d√≥lares, llamadas, ‚Ä¶).
Intuici√≥n	‚ÄúEn promedio, nos equivocamos ¬± MAE unidades por observaci√≥n.‚Äù
Ventajas	* F√°cil de explicar a negocio (no eleva errores al cuadrado).
* Penaliza linealmente los errores: cada punto cuenta igual.
Sensibilidad a outliers	Baja: un error gigante aumenta el MAE en la misma proporci√≥n que uno ‚Äúmoderado‚Äù.
Cu√°ndo preferirlo	‚Ä¢ KPI entra en planes operativos en unidades absolutas (p. ej. ¬± 20 s de AHT).
‚Ä¢ Dataset con outliers naturales que no quieres sobre‚Äìponderar.

Ejemplo r√°pido
Si MAE = 15 llamadas en un forecast horario, significa que tu pron√≥stico se desv√≠a en promedio una quinta parte de un agente (asumiendo 75 llamadas/hora por agente). Esa traducci√≥n directa es lo valioso del MAE.

2. MSE ‚Äî Mean Squared Error (Error Cuadr√°tico Medio)
Aspecto	Detalle
F√≥rmula	[
\text{MSE}= \frac{1}{n}\sum_{i=1}^{n}\bigl( y_i-\hat y_i \bigr)^{2}	
]	
Unidades	Cuadradas respecto al objetivo (si y son segundos, MSE son segundos¬≤).
Intuici√≥n	Como los errores est√°n al cuadrado, castiga m√°s los fallos grandes que los peque√±os.
Ventajas	* Matem√°ticamente conveniente: derivable, convexa para modelos lineales ‚Üí base de la regresi√≥n OLS.
* √ötil cuando quieres evitar a toda costa errores enormes.
Sensibilidad a outliers	Alta: un error √ó 10 se vuelve √ó 100 en la suma.
Cu√°ndo preferirlo	‚Ä¢ El costo de un gran error crece m√°s que linealmente (p. ej. pron√≥stico de demanda que cause ruptura de inventario).
‚Ä¢ Entrenamiento de muchos algoritmos (Regresi√≥n Lineal, XGBoost) que optimizan a priori MSE o su ra√≠z (RMSE).

RMSE = ‚àöMSE se suele reportar porque recupera las unidades originales y mantiene la penalizaci√≥n cuadr√°tica.

3. Diferencias clave MAE vs MSE
Dimensi√≥n	MAE	MSE / RMSE
Penalizaci√≥n	Lineal	Cuadr√°tica
Outliers	Robusta	Muy sensible
Unidades	Originales	Cuadradas (o originales tras ra√≠z)
Facilidad de comunicaci√≥n	Alta (¬± unidades)	Media (segundos¬≤ no son intuitivos)
Base de optimizaci√≥n	Menos usada como funci√≥n de coste (no derivable en 0)	Est√°ndar en muchas librer√≠as
Interpretaci√≥n pr√°ctica	‚ÄúPromedio de error absoluto‚Äù	‚ÄúError t√≠pico con √©nfasis en los grandes fallos‚Äù

4. ¬øCu√°l usar en la pr√°ctica?
Determina tu KPI

¬øImporta el error medio absoluto? ‚Üí MAE.

¬øEs cr√≠tico evitar picos de error? ‚Üí MSE / RMSE.

Observa la distribuci√≥n de tu variable y de los residuos.
Con heavy tails, MAE puede dar una visi√≥n m√°s estable; MSE puede dispararse.

Eval√∫a ambos al entrenar: a veces un modelo con RMSE m√°s bajo puede tener MAE m√°s alto y viceversa. Elige el que maximice el beneficio/coste real.

5. C√°lculo en Python (scikit-learn)
python
Copiar
Editar
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)  # squared=False devuelve ra√≠z
Conclusi√≥n:
MAE te dice, en las mismas unidades que tu negocio, cu√°n lejos te quedas en promedio; MSE (o su ra√≠z) enfatiza los errores grandes y es el motor matem√°tico de muchos modelos. Conocer cu√°ndo priorizar cada uno ‚Äîy poder explicarlo en t√©rminos de costo operativo‚Äî es clave en una prueba de Data Science.








T√∫ dijiste:
podrias revisar las formulas que generaste ya que no se entiende de manera clara


ChatGPT Plus
¬°Claro! Volvamos a escribir las f√≥rmulas con una notaci√≥n lo m√°s ‚Äúlimpia‚Äù y paso-a-paso posible.

1. MAE ‚Äì Mean Absolute Error (Error Absoluto Medio)
Sup√≥n que tienes n observaciones.
Para cada observaci√≥n i tienes:

Valor real‚ÄÉ ‚Üí y_i

Valor predicho ‚Üí ≈∑_i

Calcula el error individual:

ùëí
ùëñ
=
ùë¶
ùëñ
‚àí
ùë¶
^
ùëñ
e 
i
‚Äã
 =y 
i
‚Äã
 ‚àí 
y
^
‚Äã
  
i
‚Äã
 
Toma el valor absoluto (quita el signo):

‚à£
ùëí
ùëñ
‚à£
=
‚à£
ùë¶
ùëñ
‚àí
ùë¶
^
ùëñ
‚à£
‚à£e 
i
‚Äã
 ‚à£=‚à£y 
i
‚Äã
 ‚àí 
y
^
‚Äã
  
i
‚Äã
 ‚à£
Suma todos los absolutos y promedia:

MAE
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
‚ÄÖ‚Ää
‚à£
ùëí
1
‚à£
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
‚à£
ùëí
2
‚à£
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
‚Ä¶
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
‚à£
ùëí
ùëõ
‚à£
‚Äâ
ùëõ
MAE= 
n
‚à£e 
1
‚Äã
 ‚à£+‚à£e 
2
‚Äã
 ‚à£+‚Ä¶+‚à£e 
n
‚Äã
 ‚à£
‚Äã
 
2. MSE ‚Äì Mean Squared Error (Error Cuadr√°tico Medio)
Calcula el mismo error individual:

ùëí
ùëñ
=
ùë¶
ùëñ
‚àí
ùë¶
^
ùëñ
e 
i
‚Äã
 =y 
i
‚Äã
 ‚àí 
y
^
‚Äã
  
i
‚Äã
 
Ahora cuadra cada error (lo multiplicas por s√≠ mismo):

ùëí
ùëñ
‚Äâ
2
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
(
ùë¶
ùëñ
‚àí
ùë¶
^
ùëñ
)
2
e 
i
2
‚Äã
 =(y 
i
‚Äã
 ‚àí 
y
^
‚Äã
  
i
‚Äã
 ) 
2
 
Suma todos los cuadrados y promedia:

MSE
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
‚ÄÖ‚Ää
ùëí
1
2
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
ùëí
2
2
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
‚Ä¶
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
ùëí
ùëõ
2
‚Äâ
ùëõ
MSE= 
n
e 
1
2
‚Äã
 +e 
2
2
‚Äã
 +‚Ä¶+e 
n
2
‚Äã
 
‚Äã
 
Si quieres el RMSE (Root MSE) --muy usado porque devuelve las unidades originales-- simplemente tomas la ra√≠z cuadrada de la MSE:

RMSE
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
MSE
RMSE= 
MSE
‚Äã
 
Resumen r√°pido
M√©trica	Operaciones clave	Qu√© hace
MAE	Absoluto ‚Üí Promedio	Mide el error medio ‚Äútal cual‚Äù, cada fallo pesa igual.
MSE	Cuadrado ‚Üí Promedio	Penaliza m√°s los errores grandes porque 
ùëí
2
e 
2
  crece r√°pido.

Con esta notaci√≥n paso-a-paso ya no deber√≠a haber ambig√ºedad. ¬°Av√≠same si necesitas otro ejemplo num√©rico o un c√≥digo corto para calcularlas!"""

"""Pens√≥ durante 6 segundos
1. K-Nearest Neighbors (KNN) ‚Äî Concepto y mec√°nica paso a paso
¬øQu√© es?
Un m√©todo no param√©trico: no construye una ecuaci√≥n global, sino que usa el propio conjunto de entrenamiento como ‚Äúmemoria‚Äù.
En regresi√≥n, la predicci√≥n para un punto nuevo es el promedio (o promedio ponderado) de las K observaciones m√°s cercanas.

C√≥mo funciona (flujo en 4 pasos)
Paso	Detalle	Puntos clave
1 Escalar	Normaliza variables (p. ej. StandardScaler).	Distancias Euclidianas se distorsionan si los rangos difieren.
2 Elegir K	El n¬∫ de vecinos (3, 5, 10 ‚Ä¶).	‚Ä¢ K peque√±o = baja sesgo, alta varianza (overfitting).
‚Ä¢ K grande = alta suavidad, posible underfitting.
3 Calcular distancias	M√©trica t√≠pica: Euclidiana; altern.: Manhattan, Minkowski, Mahalanobis.	Se usan estructuras KD-Tree / Ball-Tree o brute-force para buscar los vecinos.
4 Agregar	Predicci√≥n = media (o media ponderada 1/distancia) de las K etiquetas.	En clasificaci√≥n, es el voto mayoritario.

python
Copiar
Editar
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

knn = Pipeline([
    ("scale", StandardScaler()),
    ("model", KNeighborsRegressor(n_neighbors=5, weights="distance"))
])
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
Ventajas
Cero coste de entrenamiento (solo guarda los datos).

Se adapta a relaciones arbitrariamente no lineales.

F√°cil de explicar: ‚Äútu valor se parece a los K m√°s parecidos‚Äù.

Limitaciones
Coste de predicci√≥n O(n √ó d) (lento con muchos puntos).

Se degrada en alta dimensi√≥n (la ‚Äúmaldici√≥n de la dimensionalidad‚Äù).

Sensible a desbalance y ruido (outliers).

Cu√°ndo usarlo en un call center
‚Äì Estimar AHT de un caso nuevo basado en tickets hist√≥ricos con caracter√≠sticas similares (tipo de problema, canal, cliente).

2. Support Vector Regression (SVR) ‚Äî Principios generales
Idea base
Extiende el concepto de Support Vector Machine (SVM) a regresi√≥n: busca una ‚Äútubo‚Äù de ancho Œµ alrededor de una funci√≥n (lineal o no) tal que

la mayor√≠a de los puntos queden dentro del tubo (error ‚â§ Œµ),

la funci√≥n sea lo m√°s plana posible (minimiza la norma de los coeficientes).

Formulaci√≥n simplificada (caso lineal)
Minimizar

1
2
‚Äâ
‚à•
ùë§
‚à•
2
‚ÄÖ‚Ää
+
‚ÄÖ‚Ää
ùê∂
‚àë
ùëñ
=
1
ùëõ
(
ùúâ
ùëñ
+
ùúâ
ùëñ
‚àó
)
2
1
‚Äã
 ‚à•w‚à• 
2
 +C 
i=1
‚àë
n
‚Äã
 (Œæ 
i
‚Äã
 +Œæ 
i
‚àó
‚Äã
 )
sujeto a

ùë¶
ùëñ
‚àí
(
ùë§
‚ä§
ùë•
ùëñ
+
ùëè
)
‚â§
¬†
ùúÄ
+
ùúâ
ùëñ
(
ùë§
‚ä§
ùë•
ùëñ
+
ùëè
)
‚àí
ùë¶
ùëñ
‚â§
¬†
ùúÄ
+
ùúâ
ùëñ
‚àó
ùúâ
ùëñ
,
ùúâ
ùëñ
‚àó
‚â•
0
y 
i
‚Äã
 ‚àí(w 
‚ä§
 x 
i
‚Äã
 +b)
(w 
‚ä§
 x 
i
‚Äã
 +b)‚àíy 
i
‚Äã
 
Œæ 
i
‚Äã
 ,Œæ 
i
‚àó
‚Äã
 
‚Äã
  
‚â§¬†Œµ+Œæ 
i
‚Äã
 
‚â§¬†Œµ+Œæ 
i
‚àó
‚Äã
 
‚â•0
‚Äã
 
w: vector de pesos (pendientes).

Œµ: tolerancia (tama√±o del tubo).

Œæ, Œæ*: slack variables (errores que caen fuera del tubo).

C: penaliza la suma de errores fuera del tubo ‚Üí controla trade-off entre suavidad y error.

N√∫cleo (kernel) y no-linealidad
Mediante el kernel trick reemplazas 
ùë•
ùëñ
‚ä§
ùë•
ùëó
x 
i
‚ä§
‚Äã
 x 
j
‚Äã
  por Œ∫(x_i,x_j) (RBF, Polin√≥mico, etc.), encontrando superficies no lineales sin trabajar en alta dimensi√≥n expl√≠cita.

python
Copiar
Editar
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

svr = Pipeline([
    ("scale", StandardScaler()),
    ("model", SVR(kernel="rbf", C=10, epsilon=0.1, gamma="auto"))
])
svr.fit(X_train, y_train)
y_pred = svr.predict(X_test)
Par√°metros esenciales
Par√°metro	Efecto	Heur√≠stica inicial
kernel	Forma de la funci√≥n (lineal, rbf, poly).	Empieza con rbf para relaciones no lineales suaves.
C	Penaliza errores > Œµ.	C peque√±o = m√°s suaves, m√°s sesgo; C grande = ajusta outliers.
epsilon	Ancho del tubo sin penalizar.	Ajusta a la tolerancia de error ‚Äúaceptable‚Äù del negocio.
gamma	(RBF) Inverso de ancho del kernel.	"auto" o 1/n_features como base; afina con Grid / Random Search.

Ventajas
Soporta relaciones muy complejas sin explotar dimensionalidad.

Se centra en ‚Äúsoportes‚Äù ‚Üí modelo sparse (solo usa unos cuantos puntos).

Control fino del bias‚Äìvariance trade-off v√≠a C y Œµ.

Desventajas
Escalado obligatorio; sensible a elecci√≥n de hiper-par√°metros.

Entrenamiento O(n¬≤): costoso con > ~10 000 muestras.

Menos interpretable que un modelo lineal o √°rbol.

Ejemplo en BPO
Prever tiempo total de gesti√≥n de un chat (incluye AHT + after-chat) donde la relaci√≥n con n√∫mero de mensajes es curva: pocos mensajes ‚Üí corto, muchos ‚Üí largo, intermedios ‚Üí impredecible. El kernel RBF captura esa forma.

Comparaci√≥n r√°pida
Aspecto	KNN	SVR
Tipo	Instance-based (lazy learn)	Algoritmo de margen (modelo‚Ää+‚Ääkernels)
Entrenamiento	O(1)	O(n¬≤)
Predicci√≥n	Lenta (dep. de n)	R√°pida una vez entrenado
Escalado	Muy necesario	Necesario
Explicabilidad	Alta (vecinos reales)	Media (puedes usar SHAP)
Uso t√≠pico	Peque√±os datasets, prototipado	Datos medianos, patrones no lineales suaves

Con estas bases puedes describir ambos algoritmos a cualquier entrevistador y elegir cu√°l aplicar seg√∫n el problema y los datos disponibles."""

"""1. ¬øQu√© es un ‚Äúbosque aleatorio‚Äù?
Un Random Forest (RF) es un ensamble que agrupa muchos √°rboles de decisi√≥n de regresi√≥n independientes y promedia sus predicciones.
Dos ‚Äúinyecciones de aleatoriedad‚Äù crean √°rboles diversos (descorrelacionados):

Bootstrap (bagging): cada √°rbol se entrena con una muestra aleatoria con reemplazo de los registros de entrenamiento.

Sub-muestra de atributos: en cada nodo, el √°rbol s√≥lo considera un subconjunto aleatorio de variables al buscar la mejor divisi√≥n.

El algoritmo final hace:

ùë¶
^
RF
(
ùë•
)
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
1
ùëá
‚àë
ùë°
=
1
ùëá
‚ÄÖ‚Ää
ùë¶
^
(
ùë°
)
(
ùë•
)
y
‚Äã
  
RF
‚Äã
 (x)= 
T
1
‚Äã
  
t=1
‚àë
T
‚Äã
  
y
‚Äã
  
(t)
 (x)
donde 
ùëá
T es el n√∫mero de √°rboles y 
ùë¶
^
(
ùë°
)
y
‚Äã
  
(t)
  la predicci√≥n del √°rbol t.

Intuici√≥n: cada √°rbol es un ‚Äújuez‚Äù con un punto de vista sesgado pero distinto; la media de todos reduce las excentricidades individuales (varianza) sin aumentar demasiado el sesgo.

2. Ventajas clave frente a la regresi√≥n lineal
Dimensi√≥n	Random Forest	Regresi√≥n lineal
Forma de la relaci√≥n	Capta no linealidad y interacciones complejas autom√°ticamente.	Supone relaci√≥n puramente lineal a menos de que agregues t√©rminos manualmente (x¬≤, log x‚Ä¶).
Escalado de variables	No requiere normalizar ni estandarizar.	Coeficientes sensibles a escalas; se suele estandarizar.
Outliers	Menos afectado (cada divisi√≥n s√≥lo mira su rama).	Outliers pueden sesgar la l√≠nea global.
Feature engineering	Detecta divisiones √≥ptimas sin que el analista cree features polin√≥micas.	Necesitas fabricar features para capturar curvaturas.
Robustez a multicolinealidad	Alta; variables redundantes compiten en splits y se diluyen.	Coeficientes inestables si las X est√°n muy correlacionadas.
Overfitting	Controlado por n_estimators, max_depth, min_samples_leaf.	Puede sub-o over-fit, pero es menos flexible por naturaleza.
Interpretabilidad	‚ÄúCaja gris‚Äù: se explica via importancia de variables o SHAP.	Coeficientes directos Œ≤ ‚Üí efecto marginal claro.

Conclusi√≥n: si sospechas relaciones curvas o interacciones imposibles de linealizar f√°cilmente (p. ej. tiempo de espera √ó tipo de llamada), RF suele ofrecer mejor performance con poca afinaci√≥n.

3. Importancia de variables (feature importance)
Random Forest proporciona dos m√©tricas globales; ambas responden ‚Äú¬øcu√°nto aporta cada variable al poder predictivo del bosque?‚Äù

Tipo	C√≥mo se calcula	Pros / Contras
Impureza (MDI ‚Äì Mean Decrease in Impurity)	Para cada split de un √°rbol t calcula la reducci√≥n de MSE. Suma y promedia sobre todos los √°rboles.	R√°pido (viene gratis al entrenar); sesgo hacia variables con m√°s categor√≠as o alta cardinalidad.
Permutaci√≥n (MDA ‚Äì Mean Decrease in Accuracy)	Para cada variable: baraja sus valores en el conjunto val, mide cu√°nto empeora el MAE/RMSE.	Model-agnostic, corrige sesgo de cardinalidad; m√°s lento (requiere pasar muchas veces por el bosque).

Ejemplo pr√°ctico en scikit-learn
python
Copiar
Editar
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import pandas as pd

rf = RandomForestRegressor(
        n_estimators=400,
        max_depth=None,
        random_state=42
).fit(X_train, y_train)

# 1) Importancia por impureza
imp_mdi = pd.Series(rf.feature_importances_, index=X_train.columns) \
             .sort_values(ascending=False)

# 2) Importancia por permutaci√≥n
result = permutation_importance(rf, X_val, y_val,
                                n_repeats=10, random_state=42)
imp_mda = pd.Series(result.importances_mean, index=X_train.columns) \
             .sort_values(ascending=False)

print("Top-5 MDI\n", imp_mdi.head())
print("Top-5 MDA\n", imp_mda.head())
C√≥mo interpretarlo para negocio

‚Äúivr_steps explica el 31 % de la reducci√≥n total de error; si aumentaran los pasos en el IVR, esperar√≠amos un salto en la duraci√≥n de llamada.
agent_tenure contribuye un 18 %: agentes nuevos incrementan el AHT. Esto confirma que invertir en entrenamiento reducir√° costos.‚Äù

Para explicaciones fila-a-fila (predicciones individuales), complementa con SHAP:

python
Copiar
Editar
import shap
expl = shap.TreeExplainer(rf)
shap_values = expl.shap_values(X_val.iloc[0])     # un solo caso
shap.waterfall(shap.Explanation(shap_values,
                                 expl.expected_value,
                                 X_val.iloc[0]))
4. Par√°metros esenciales a afinar
Hiper-par√°metro	Influencia	Heur√≠stica inicial
n_estimators	N¬∫ de √°rboles: ‚Üë ‚Üí ‚Üì varianza, ‚Üë costo.	300-500 en datasets < 100 k filas.
max_depth	Profundidad m√°xima de cada √°rbol.	None (deja que crezca) + min_samples_leaf ‚â• 5.
min_samples_leaf	M√≠n. de registros por hoja ‚Üí suaviza predicci√≥n.	1 % de n si hay mucho ruido.
max_features	N¬∫ de variables candidatas por split.	auto (= ‚àöd) en regresi√≥n.

Para obtener los valores √≥ptimos ‚Üí Grid/Random Search con validaci√≥n cruzada (n_jobs=-1 para paralelizar).

5. Cu√°ndo no usar Random Forest
Necesitas predicciones fuera de rango (extrapolaci√≥n) ‚Üí RF s√≥lo interpola dentro del dominio visto.

~> 1 M filas + > 200 features ‚Üí RAM y tiempo crecen; considera Gradient Boosting o XGBoost con histogram-based splits.

Requieres coeficientes claros (elasticidad en %) ‚Üí usa regresi√≥n (lineal, Poisson) o GAMs."""

"""Clasificaci√≥n vs Regresi√≥n ‚Äî las dos grandes familias de problemas supervisados
Dimensi√≥n	Clasificaci√≥n	Regresi√≥n
Qu√© predice	Etiquetas discretas (clases).
Ej.: ‚Äúchurn / no-churn‚Äù, ‚Äútipo de llamada: billing, tech, ventas‚Äù.	Valores continuos (num√©ricos).
Ej.: duraci√≥n de llamada en segundos, monto de facturaci√≥n.
Salida t√≠pica del modelo	- Probabilidades por clase (P(clase=k))
- Etiqueta final (la de mayor prob.)	- N√∫mero real (≈∂ = 237 s de AHT).
Funci√≥n de p√©rdida	Cross-entropy, hinge, log loss, etc.	MSE, MAE, Huber, Poisson deviance.
M√©tricas de evaluaci√≥n	Accuracy, ROC-AUC, F1, Precision/Recall, Kappa, Log-loss.	MAE, RMSE, R¬≤, MAPE, sMAPE.
Modelos m√°s usados	Logistic Regression, Random Forest Classifier, XGBoost Classifier, SVM, Redes neuronales de clasificaci√≥n.	Linear/Polynomial Regression, Random Forest Regressor, Gradient Boosting Regressor (XGB/LGBM), SVR, Redes neuronales de regresi√≥n.
Curva de decisi√≥n	Frontera que separa regiones de clase en el espacio de variables.	Superficie (curva) que aproxima la relaci√≥n X ‚Üí Y.
Umbral cr√≠tico	S√≠: se fija un threshold sobre la probabilidad para decir ¬´clase=1¬ª.	No: se predice directamente el valor; el negocio define m√°rgenes de error aceptables.
Coste t√≠pico de error	Asim√©trico (FN vs FP pueden costar distinto).	Generalmente proporcional a la magnitud del error, o cuadr√°tico (RMSE).
Ejemplo en un call center	Detectar si una llamada acabar√° insatisfecha (CSAT bajo) ‚Üí bandera roja al supervisor.	Estimar cu√°ntos segundos tomar√° resolver la pr√≥xima llamada para dimensionar staffing.

Pistas r√°pidas para la entrevista
¬øQu√© pasa si convierto un problema continuo en categ√≥rico?
‚Äì Pierdes informaci√≥n (duraci√≥n 179 s ‚âà 181 s) y precisas m√°s datos para igual precisi√≥n.

¬øPuedo usar un modelo de regresi√≥n para clasificaci√≥n?
‚Äì S√≠, si conviertes la regresi√≥n a un scoring y aplicas un umbral, pero no optimizar√°s m√©tricas de clasificaci√≥n de forma directa.

¬øC√≥mo elijo m√©trica?
‚Äì Clasificaci√≥n: depende del balance entre FP y FN (p. ej. ROC-AUC para ranking, F1 si la clase positiva es rara).
‚Äì Regresi√≥n: MAE si el negocio entiende unidades absolutas; RMSE si penalizas mucho los grandes errores.

¬øSe pueden combinar?
‚Äì Modelos mixtos (multi-output): p.ej. predecir clase de ticket y tiempo de resoluci√≥n en un solo pipeline.

Con esto podr√°s explicar claramente la diferencia de naturaleza, objetivos y m√©tricas de cada tipo de problema en Data Science."""

"""¬øQu√© es el criterio de parada en un √°rbol de decisi√≥n?
Cuando entrenamos un √°rbol, el algoritmo va dividiendo el conjunto de datos en nodos cada vez m√°s puros (menos mezcla de clases o menor varianza, seg√∫n sea clasificaci√≥n o regresi√≥n).
Si lo dej√°ramos crecer sin control, acabar√≠amos con:

Hojas de un solo registro (error casi 0 en entrenamiento, pero alt√≠simo riesgo de overfitting).

Tiempo y memoria innecesarios.

El criterio de parada define cu√°ndo dejar de dividir un nodo y declarar que es hoja definitiva.
En otras palabras: ‚Äú¬øHasta qu√© punto vale la pena seguir complicando el √°rbol?‚Äù

Criterios de parada m√°s comunes
Par√°metro (en scikit-learn)	Qu√© controla	Ejemplo t√≠pico
max_depth	Profundidad m√°xima (niveles).	max_depth=6 limita el √°rbol a 6 decisiones en cadena.
min_samples_split	M√≠nimo de registros para intentar un nuevo split.	min_samples_split=20 ‚Üí si el nodo tiene 19 o menos, se vuelve hoja.
min_samples_leaf	M√≠nimo de registros que debe contener cada hoja resultante.	min_samples_leaf=5 evita hojas diminutas.
min_impurity_decrease	Umbral m√≠nimo de reducci√≥n de impureza (Gini, Entrop√≠a, MSE) para aceptar un split.	min_impurity_decrease=0.001 requiere ganancia ‚Äúsignificativa‚Äù.
max_leaf_nodes	L√≠mite global de hojas.	max_leaf_nodes=50 fuerza poda temprana.
min_weight_fraction_leaf	Igual que min_samples_leaf pero proporcional al total (√∫til con sample weights).	0.02 ‚áí cada hoja ‚â• 2 % del peso total.

En √°rboles de regresi√≥n, ‚Äúimpureza‚Äù = varianza (MSE).
En clasificaci√≥n, puede ser Gini o Entrop√≠a.

¬øC√≥mo interact√∫an?
El algoritmo intenta todos los splits posibles para la variable candidata.

Calcula la reducci√≥n de impureza.

S√≥lo acepta el split si cumple todos los requisitos (ej.: produce ‚â• min_samples_leaf, profundidad < max_depth, y la ganancia supera min_impurity_decrease).

Si ning√∫n split v√°lido existe, el nodo se declara hoja y ah√≠ termina su rama.

¬øPor qu√© es crucial?
Evitar overfitting: Limitar la profundidad o exigir un tama√±o m√≠nimo de hoja impide que el √°rbol ‚Äúmemorice‚Äù ruido.

Generalizaci√≥n y estabilidad: Un √°rbol muy ajustado var√≠a mucho si cambias ligeramente los datos (alta varianza).

Eficiencia computacional: Menos nodos = modelo m√°s ligero y predicciones m√°s r√°pidas.

Interpretabilidad: √Årboles m√°s compactos son m√°s f√°ciles de explicar a negocio.

Ejemplo r√°pido en Python
python
Copiar
Editar
from sklearn.tree import DecisionTreeRegressor

tree = DecisionTreeRegressor(
    max_depth=6,
    min_samples_leaf=10,
    min_impurity_decrease=0.0005,
    random_state=42
).fit(X_train, y_train)
Con esos hiperpar√°metros:

Nunca habr√° m√°s de 6 saltos desde la ra√≠z a una hoja.

Cada hoja contendr√° ‚â• 10 registros.

Un split se acepta s√≥lo si reduce la varianza en ‚â• 0.0005.

Resumen en una frase
El criterio de parada es el conjunto de reglas que le dice al algoritmo de √°rbol ‚Äú¬°basta de dividir aqu√≠!‚Äù, equilibrando precisi√≥n y simplicidad para que el modelo sea √∫til y no se sobreajuste."""

"""‚ÄúKit estad√≠stico‚Äù esencial para un data scientist
(Conceptos explicados en lenguaje llano + recordatorio de c√≥mo se calculan)

1. Descriptivos b√°sicos - ‚Äúfotograf√≠a‚Äù inicial del dato
C√°lculo	Qu√© mide	C√≥mo se obtiene (idea)
Media (promedio)	El centro ‚Äúequilibrado‚Äù.	Suma de valores √∑ n.
Mediana	Punto que corta la serie al 50 %.	Ordenas y tomas el del medio.
Moda	Valor m√°s repetido.	Cuenta apariciones.
Rango	Extensi√≥n total.	M√°x ‚Äì m√≠n.
Varianza (œÉ¬≤) / Desviaci√≥n est√°ndar (œÉ)	Qu√© tan dispersos est√°n.	Promedio de (valor ‚àí media)¬≤; ‚àö para œÉ.
IQR (rango intercuart√≠lico)	Dispersi√≥n robusta a outliers.	Q3 ‚Äì Q1 (75 % ‚Äì 25 %).
Skewness	Asimetr√≠a (cola a la derecha/izq.).	F√≥rmula de 3er momento estandarizado.
Kurtosis	‚ÄúPicos‚Äù y colas.	4¬∫ momento estandarizado.

2. Probabilidad & distribuciones - ‚Äúcomportamiento esperado‚Äù
Bernoulli / Binomial: √©xitos-fracasos (clic/no clic).

Poisson: recuentos por intervalo (llamadas por minuto).

Normal: muchas medias y errores tienden a ella (CLT).
‚Äì C√°lculo √∫til: z-score = (valor ‚àí media)/œÉ ‚Üí cu√°ntas œÉ te alejas.

Exponencial: tiempo entre eventos independientes.

Uniforme: todos los valores del rango igual probabilidad.

3. Muestreo y el ‚Äúerror natural‚Äù
Concepto	Para qu√© sirve
Tama√±o de muestra (n)	Balancea coste vs precisi√≥n. F√≥rmulas dependen de variabilidad y confianza.
Error est√°ndar (SE)	Desviaci√≥n de la media de muestras. SE = œÉ/‚àön.
Teorema del l√≠mite central (CLT)	La distribuci√≥n de la media ‚Üí normal si n es ‚Äúgrande‚Äù, sin importar la forma original.
Bootstrap	Re-muestreo con reemplazo para estimar incertidumbre cuando las f√≥rmulas no aplican.

4. Inferencia: convertir muestra ‚Üí conclusiones
Herramienta	Para qu√©	Regla r√°pida
Intervalo de confianza (IC)	Rango plausible para el par√°metro.	Media ¬± (se √ó z_{Œ±/2}).
Pruebas t / z	¬øMedia ‚â† valor H‚ÇÄ? / ¬øMedias distintas?	Si p-value < Œ± (0.05) ‚Üí ‚Äúdiferencia significativa‚Äù.
Chi-cuadrado	Tablas de frecuencias, bondad de ajuste.	
ANOVA	>2 medias de grupos. F grande ‚Üí al menos una media difiere.	
Tests no param√©tricos (Mann-Whitney, Wilcoxon)	Igualdad de medianas sin asumir normalidad.	
Valor-p	Probabilidad de ver datos ‚â• extremos asumiendo H‚ÇÄ. Peque√±o ‚Üí evidencia contra H‚ÇÄ.	
Tama√±o de efecto (Cohen d, r)	‚ÄúCu√°n grande‚Äù es la diferencia, no s√≥lo si existe.	

5. Relaciones entre variables
Covarianza: direcci√≥n de la relaci√≥n (dif√≠cil de leer por unidades).

Correlaci√≥n (Pearson r): fuerza lineal entre ‚àí1 y 1.
Spearman œÅ si la relaci√≥n es mon√≥tona pero no lineal.

Regresi√≥n lineal: ajusta yÃÇ = Œ≤‚ÇÄ + Œ≤‚ÇÅx.
‚Äì Œ≤‚ÇÅ: cambio esperado en y por unidad de x.
‚Äì R¬≤: % de varianza explicada.

√çndice VIF: detecta multicolinealidad en regresores.

6. Evaluaci√≥n de modelos (ver√°s estos nombres en ML)
M√©trica	Tipo de problema	Intuici√≥n
MAE / MSE / RMSE	Regresi√≥n	Error medio (absoluto o cuadr√°tico).
R¬≤ ajustado	Regresi√≥n	Varianza explicada penalizando n¬∫ de features.
Accuracy, Precision, Recall, F1	Clasificaci√≥n	Aciertos generales vs aciertos clase positiva.
ROC-AUC / PR-AUC	Clasificaci√≥n desequilibrada	Capacidad de ranking entre clases.
Log Loss / Cross-Entropy	Probabil√≠stico	Penaliza predicciones seguras y equivocadas.

7. Otros ‚Äúimperdibles‚Äù
Bayes: Posterior ‚àù Prior √ó Verosimilitud ‚Üí actualiza creencias.

AIC / BIC: comparan modelos penalizando complejidad.

Autocorrelaci√≥n & ACF: dependencia temporal en series.

Durbin-Watson: detecta autocorrelaci√≥n de residuos.

C√≥mo estudiarlos r√°pido
Descriptivos ‚Üí practica pandas.describe() y f√≥rmulas manuales.

Inferencia ‚Üí simula coin-toss en Python y verifica IC y valor-p.

Regresi√≥n & Correlaci√≥n ‚Üí usa scipy.stats.linregress.

Evaluaci√≥n modelos ‚Üí sklearn.metrics tiene casi todas las m√©tricas.

Con este glosario tendr√°s los c√°lculos estad√≠sticos indispensables dominados para cualquier rol de Data Science. Si quieres ejemplos de c√≥digo o ejercicios concretos de alg√∫n apartado, ¬°av√≠same!"""
"""
C√≥mo se calcula R¬≤ (coeficiente de determinaci√≥n)
‚Ä¶y qu√© significa en una regresi√≥n lineal simple
1. Concepto intuitivo üö¶
R¬≤ responde a esta pregunta:

¬øQu√© proporci√≥n de la variabilidad de y (la variable que quieres predecir) es explicada por la recta que has ajustado con x?

Valor-gu√≠a*

0 ‚Üí la recta no explica nada; predecir√≠as igual usando la media de y.

1 ‚Üí la recta explica el 100 % de la variaci√≥n (todos los puntos caen exactamente sobre la l√≠nea).

< 0 ‚Üí (s√≥lo aparece en predicci√≥n fuera de muestra) el modelo es peor que usar la media.

2. F√≥rmula paso a paso
Imagina que tienes n pares 
(
ùë•
ùëñ
,
ùë¶
ùëñ
)
(x 
i
‚Äã
 ,y 
i
‚Äã
 ) y has ajustado la recta

ùë¶
^
ùëñ
=
ùõΩ
0
+
ùõΩ
1
ùë•
ùëñ
y
^
‚Äã
  
i
‚Äã
 =Œ≤ 
0
‚Äã
 +Œ≤ 
1
‚Äã
 x 
i
‚Äã
 
Varianza total de y (‚Äúcu√°nto se mueven los valores reales‚Äù)

SST
=
‚àë
ùëñ
=
1
ùëõ
‚Äâ
(
ùë¶
ùëñ
‚àí
ùë¶
Àâ
)
2
SST= 
i=1
‚àë
n
‚Äã
 (y 
i
‚Äã
 ‚àí 
y
Àâ
‚Äã
 ) 
2
 
Errores del modelo (‚Äúlo que la recta no acierta‚Äù)

SSR
=
‚àë
ùëñ
=
1
ùëõ
‚Äâ
(
ùë¶
ùëñ
‚àí
ùë¶
^
ùëñ
)
2
SSR= 
i=1
‚àë
n
‚Äã
 (y 
i
‚Äã
 ‚àí 
y
^
‚Äã
  
i
‚Äã
 ) 
2
 
R¬≤

ùëÖ
2
=
1
‚ÄÖ‚Ää
‚àí
‚ÄÖ‚Ää
SSR
SST
R 
2
 =1‚àí 
SST
SSR
‚Äã
 
Interpretaci√≥n:

Si la recta reduce mucho SSR respecto a SST, el cociente es peque√±o y R¬≤ se acerca a 1.

Si la recta apenas mejora la media, SSR ‚âà SST y R¬≤ ‚âà 0.

3. Ejemplo num√©rico r√°pido
ùë•
x	
ùë¶
y	
ùë¶
^
y
^
‚Äã
  (recta)
1	2	2.2
2	3	3.1
3	2	4.0

ùë¶
Àâ
=
(
2
+
3
+
2
)
/
3
=
2.33
y
Àâ
‚Äã
 =(2+3+2)/3=2.33

SST = 
(
2
‚àí
2.33
)
2
+
(
3
‚àí
2.33
)
2
+
(
2
‚àí
2.33
)
2
=
0.11
+
0.44
+
0.11
=
0.66
(2‚àí2.33) 
2
 +(3‚àí2.33) 
2
 +(2‚àí2.33) 
2
 =0.11+0.44+0.11=0.66

SSR = 
(
2
‚àí
2.2
)
2
+
(
3
‚àí
3.1
)
2
+
(
2
‚àí
4.0
)
2
=
0.04
+
0.01
+
4
=
4.05
(2‚àí2.2) 
2
 +(3‚àí3.1) 
2
 +(2‚àí4.0) 
2
 =0.04+0.01+4=4.05

ùëÖ
2
=
1
‚àí
4.05
0.66
‚âà
‚àí
5.13
R 
2
 =1‚àí 
0.66
4.05
‚Äã
 ‚âà‚àí5.13
R¬≤ es negativo: fuera de muestra la recta lo hizo peor que la simple media (caso inventado para que se vea el concepto).

4. C√°lculo en Python (scikit-learn)
python
Copiar
Editar
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import numpy as np

X = np.array([[1],[2],[3]])
y = np.array([2,3,2])

model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

print("R¬≤:", r2_score(y, y_pred))   # coincide con model.score(X, y)
5. Matices importantes
R¬≤ alto no = buen modelo: puede estar sobre-ajustado (f√≠jate en R¬≤ sobre datos de validaci√≥n).

Comparaci√≥n de modelos con distinto n¬∫ de variables: usa R¬≤ ajustado para penalizar el exceso de features.

ùëÖ
adj
2
=
1
‚àí
(
1
‚àí
ùëÖ
2
)
‚Äâ
ùëõ
‚àí
1
ùëõ
‚àí
ùëù
‚àí
1
R 
adj
2
‚Äã
 =1‚àí(1‚àíR 
2
 ) 
n‚àíp‚àí1
n‚àí1
‚Äã
 
donde 
ùëù
p = n.¬∫ de predictores.

Unidades: R¬≤ es adimensional; siempre entre ‚àí‚àû y 1.

Relaci√≥n no lineal: un R¬≤ bajo puede indicar que la relaci√≥n no es lineal, no necesariamente que ‚Äúno exista‚Äù.

Resumen en 1 frase
R¬≤ = 1 ‚àí (errores de la recta √∑ variaci√≥n total); indica la fracci√≥n de la dispersi√≥n de y que tu modelo lineal logra capturar."""

"""¬øC√≥mo gana dinero y opera un call center/BPO en Colombia?
Caso referencia: GroupCOS
1. Propuesta de valor
Externalizaci√≥n integral: asume procesos de atenci√≥n, ventas y back-office que no son core para el cliente (telecom, retail, fintech, salud).

Nearshoring: presta servicio en espa√±ol e ingl√©s desde Colombia y M√©xico ‚Äî huso horario compatible con EE. UU. y Canad√°, y costos ~50 % menores que on-shore. 
latam.groupcosbpo.com.co
Connext

Enfoque ‚Äútech + talento‚Äù: combina plataformas de contact center omnicanal, RPA y anal√≠tica con agentes certificados. 
groupcosbpo.com
LinkedIn

Calidad certificada: PCI-DSS, ISO 27001, Friendly Biz, etc.; tranquiliza a clientes de sectores regulados. 
Comercial Group COS -

2. L√≠neas de servicio y flujos de ingresos
L√≠nea	Qu√© vende	C√≥mo se tarifica*
Inbound CX (soporte, post-venta)	Horas/agente o ‚ÄúFTE‚Äù + bono por SLA (TMO, FCR).	$/hora o $/FTE/mes
Outbound ventas/cobranza	Contacto a leads, recordatorios de pago.	$/venta cerrada o comisi√≥n %
BPO back-office	Digitaci√≥n, KYC, verificaci√≥n, chat moderaci√≥n.	$/transacci√≥n
RPA & anal√≠tica	Bots, speech-to-text, dashboards (arriba de Vicidial, Genesys, 3CX, etc.).	Proyecto cerrado + fee de mantenimiento
Nearshore staffing	Equipos dedicados (tech-support lvl 1‚Äì2).	Costo + margen (modelo seat leasing)

*Los contratos suelen mezclar tarifas fijas + variables por KPI.

3. Cost structure y m√°rgenes t√≠picos
Rubro	% del costo total aprox.	Gesti√≥n en GroupCOS
Talento (salario + prestaciones)	55‚Äì65 %	Reclutamiento continuo, academia interna ‚Äì Learning & Development.
Tecnolog√≠a (licencias CCaaS, CRM, RPA)	10‚Äì15 %	Alianzas con Avaya, Zendesk; bots propios.
Infraestructura (sites, energ√≠a, seguridad)	8‚Äì10 %	Sedes Bogot√°, Bucaramanga; redundancia TIER III.
QA & compliance	4‚Äì6 %	Auditor√≠a 100 % de llamadas con speech-analytics.
G&A	10‚Äì15 %	Direcci√≥n, ventas, certificaciones.

M√°rgenes EBITDA para players medianos en Colombia: 12‚Äì18 %; se eleva con automatizaci√≥n (bots atienden ‚âà30 % de interacciones sin agente).

4. Operaci√≥n diaria (modelo Delivery)
Discovery ‚Üí diagn√≥stico del proceso del cliente.

Blueprint & KPIs ‚Üí definici√≥n de SLA (AHT, CSAT, NPS, ventas/hr).

Ramp-up ‚Üí reclutamiento, entrenamiento, prueba piloto.

Run & Monitor ‚Üí reporting en tiempo real; metodolog√≠a interna de mejora (Lean + Design Thinking). 
groupcosbpo.com

Continuous Improvement ‚Üí an√°lisis de voz, dashboards Power BI, retrain ML.

5. Diferenciales frente a otros call centers
Factor	C√≥mo lo capitaliza GroupCOS
Tecnolog√≠a propietaria	RPA y speech-analytics propios para reducir AHT.
Certificaciones	Abre puertas a bancos/fintech que exigen ISO 27001, PCI-DSS.
Multipa√≠s	‚ÄúFollow-the-sun‚Äù con sites Colombia‚ÄìM√©xico; cobertura 24/7 sin turnos extremos.
Cultura biling√ºe	Agentes con B2‚ÄìC1 permiten subir tarifas en USD.

6. Indicadores de √©xito que reporta al cliente
Costo por contacto vs baseline.

AHT y FCR (First-Call Resolution).

CSAT / NPS mensual.

Ventas incremental por hora en campa√±as outbound.

ROI de RPA (horas-hombre liberadas).

Conclusi√≥n
El negocio de GroupCOS (y call centers BPO similares en Colombia) se basa en monetizar talento local + diferencial de costos + tecnolog√≠a propia para entregar atenci√≥n y procesos back-office a empresas globales. El ingreso combina tarifas por agente/ transacci√≥n y bonos por desempe√±o; el √©xito depende de escalar equipos r√°pido, mantener calidad certificada y, cada vez m√°s, automatizar para proteger m√°rgenes."""